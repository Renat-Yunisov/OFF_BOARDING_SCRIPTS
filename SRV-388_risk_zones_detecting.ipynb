{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbadd08b",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c774c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "import shap\n",
    "\n",
    "\n",
    "# Data analysis / Data processing\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "from datetime import time, timedelta, datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from ydata_profiling import ProfileReport\n",
    "import re\n",
    "from typing import Set, List\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Maths & Stats\n",
    "import math \n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "import AB_library\n",
    "import random\n",
    "\n",
    "# System library\n",
    "import os\n",
    "import ipywidgets\n",
    "import warnings\n",
    "import pandas_gbq\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# from itables import init_notebook_mode\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "import openpyxl\n",
    "\n",
    "# Data connection\n",
    "from google.cloud import bigquery\n",
    "bigquery_client = bigquery.Client(project='analytics-dev-333113')\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import typing as t\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from enum import Enum\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import GenerativeModel, Image\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "# Useful functions\n",
    "def cycle_sql(start, end, query, weeks=False):\n",
    "    \"\"\"\n",
    "    You have to use {date} in your script to add cycle date into this backets\n",
    "    \"\"\"\n",
    "    date_start = datetime.strptime(start, '%Y-%m-%d')\n",
    "    date_end = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "    if weeks == False:\n",
    "        daterange = [(date_start + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days)+1)]\n",
    "    else:\n",
    "        daterange = [(date_start + timedelta(weeks=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days//7)+1)] # weeks dividing days by 7\n",
    "\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for date in daterange:\n",
    "        counter+=1\n",
    "        print(f\"{counter}) Uploading - {date}:\", datetime.today().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        script = query.format(date = date)\n",
    "        df_cycle = bigquery_client.query(script).to_dataframe()\n",
    "        if df_cycle.empty == True:\n",
    "            print('Dataframe is empty')\n",
    "        total_df = pd.concat([df_cycle, total_df])\n",
    "    return total_df\n",
    "\n",
    "def read_bq(query, project='analytics-dev-333113'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    query_job = client.query(query)\n",
    "    result_df = query_job.to_dataframe()\n",
    "    return result_df\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(\n",
    "        html_str.replace('table','table style=\"display:inline\"'), \n",
    "        raw=True\n",
    "    )\n",
    "\n",
    "def writing_excel(name:str, dataset1=None, dataset2=None, dataset3=None, dataset4=None):\n",
    "    with pd.ExcelWriter(f\"{name}.xlsx\") as writer:\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "\n",
    "        if dataset1 is not None:\n",
    "            if dataset2 is not None:\n",
    "                if dataset3 is not None:\n",
    "                    if dataset4 is not None:\n",
    "                        dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset3.to_excel(writer, sheet_name=f\"3-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset4.to_excel(writer, sheet_name=f\"4-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                    else:\n",
    "                        dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset3.to_excel(writer, sheet_name=f\"3-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                else:\n",
    "                    dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                    #   index=False\n",
    "                                        )\n",
    "                    dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                    #   index=False\n",
    "                                        )\n",
    "            else:\n",
    "                dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                #   index=False\n",
    "                                    )\n",
    "\n",
    "        print('DataFrame is written to Excel File successfully.') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1512f05",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "cceb3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH last_incident AS (SELECT redmine_id     AS id_last_incident,\n",
    "                              city_id,\n",
    "                              city_name,\n",
    "                              country_name,\n",
    "                              macroregion_name,\n",
    "                              from_latitude  AS main_from_latitude,\n",
    "                              from_longitude AS main_from_longitude,\n",
    "                              from_address   AS main_from_address\n",
    "                       FROM indriver-bi.safety.vw_safety_incidents_detail\n",
    "                       WHERE 1 = 1\n",
    "                         AND incident_date BETWEEN DATE_ADD(CURRENT_DATE(), INTERVAL -15 DAY) AND DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY)\n",
    "                         AND information_status IN ('Confirmed')\n",
    "                       QUALIFY ROW_NUMBER() OVER (PARTITION BY city_id ORDER BY incident_date DESC) = 1),\n",
    "     same_location_incidents AS (SELECT redmine_id,\n",
    "                                        pass_id,\n",
    "                                        driver_id,\n",
    "                                        incident_date,\n",
    "                                        information_status,\n",
    "                                        incident_level,\n",
    "                                        incident_type,\n",
    "                                        city_id AS city_id_same_loc,\n",
    "                                        from_latitude,\n",
    "                                        from_longitude,\n",
    "                                        from_address\n",
    "                                 FROM indriver-bi.safety.vw_safety_incidents_detail\n",
    "                                 WHERE 1 = 1\n",
    "                                   AND incident_date BETWEEN DATE_ADD(CURRENT_DATE(), INTERVAL -30 DAY) AND DATE_ADD(CURRENT_DATE(), INTERVAL -1 DAY)\n",
    "                                   AND information_status IN ('Not confirmed', 'Confirmed', 'Automated ML decision')\n",
    "                                   AND (from_longitude IS NOT NULL AND from_latitude IS NOT NULL)),\n",
    "     aggregated AS (SELECT *,\n",
    "                           ST_DISTANCE(\n",
    "                                   ST_GEOGPOINT(t1.main_from_longitude, t1.main_from_latitude),\n",
    "                                   ST_GEOGPOINT(t2.from_longitude, t2.from_latitude)\n",
    "                           ) AS              distance\n",
    "                    FROM last_incident t1\n",
    "                             LEFT JOIN same_location_incidents t2\n",
    "                                       ON t1.city_id = t2.city_id_same_loc \n",
    "                    ORDER BY distance)\n",
    "SELECT city_id,\n",
    "       city_name,\n",
    "       country_name,\n",
    "       macroregion_name,\n",
    "       id_last_incident                              AS main_incident_id,\n",
    "       main_from_latitude,\n",
    "       main_from_longitude,\n",
    "       main_from_address,\n",
    "       redmine_id                                    AS near_incident_id,\n",
    "       incident_date                                 AS near_incident_date,\n",
    "       CURRENT_DATE()                                AS current_date,\n",
    "       DATE_DIFF(CURRENT_DATE(), incident_date, DAY) AS date_diff,\n",
    "       from_latitude                                 AS near_from_latitude,\n",
    "       from_longitude                                AS near_from_longitude,\n",
    "       from_address                                  AS near_from_address,\n",
    "       IF(information_status IN ('Confirmed', 'Automated ML decision'), 'Confirmed',\n",
    "          'Not confirmed')                           AS information_status,\n",
    "       incident_level                                AS incident_level,\n",
    "       incident_type                                 AS incident_type,\n",
    "       distance                                      AS distance\n",
    "FROM aggregated\n",
    "WHERE 1 = 1\n",
    "  AND city_id_same_loc IS NOT NULL\n",
    "  AND distance IS NOT NULL\n",
    "  AND distance <= 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "76f8b3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>incident_level</th>\n",
       "      <th>city_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>country_name</th>\n",
       "      <th>macroregion_name</th>\n",
       "      <th>main_from_latitude</th>\n",
       "      <th>main_incident_id</th>\n",
       "      <th>main_from_longitude</th>\n",
       "      <th>main_from_address</th>\n",
       "      <th>information_status</th>\n",
       "      <th>Green</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "      <th>total</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>4258</td>\n",
       "      <td>Arequipa</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>-16.392</td>\n",
       "      <td>SQ-1041094</td>\n",
       "      <td>-71.54</td>\n",
       "      <td>Cl√≠nica Arequipa</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>-16.392 -71.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4199</td>\n",
       "      <td>Lima</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>-12.057</td>\n",
       "      <td>SQ-1040967</td>\n",
       "      <td>-77.001</td>\n",
       "      <td>Pje. Plut√≥n 160</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>-12.057 -77.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>4300</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>Africa</td>\n",
       "      <td>-33.991</td>\n",
       "      <td>SQ-1042637</td>\n",
       "      <td>18.567</td>\n",
       "      <td>Sokhanyo Public Primary School</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>-33.991 18.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4257</td>\n",
       "      <td>Trujillo</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>-8.124</td>\n",
       "      <td>SQ-1036866</td>\n",
       "      <td>-79.038</td>\n",
       "      <td>Mi Facultad Bar</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>-8.124 -79.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>4243</td>\n",
       "      <td>Barranquilla</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>10.983</td>\n",
       "      <td>SQ-1042334</td>\n",
       "      <td>-74.784</td>\n",
       "      <td>Baranoa</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>10.983 -74.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>4825</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>18.482</td>\n",
       "      <td>SQ-1042097</td>\n",
       "      <td>-69.83</td>\n",
       "      <td>C. Cam. de Juan L√≥pez 25</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>18.482 -69.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>4275</td>\n",
       "      <td>Piura</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>-5.187</td>\n",
       "      <td>SQ-1030971</td>\n",
       "      <td>-80.623</td>\n",
       "      <td>Hospital Regional Jos√© Cayetano Heredia</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>-5.187 -80.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>5388</td>\n",
       "      <td>Lahore</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>SA</td>\n",
       "      <td>31.448</td>\n",
       "      <td>SQ-1040766</td>\n",
       "      <td>74.259</td>\n",
       "      <td>Ayesha Girls Hostel</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>31.448 74.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>5548</td>\n",
       "      <td>Panama</td>\n",
       "      <td>Panama</td>\n",
       "      <td>Latin America</td>\n",
       "      <td>8.953</td>\n",
       "      <td>SQ-1041961</td>\n",
       "      <td>-79.533</td>\n",
       "      <td>C. 3a Este</td>\n",
       "      <td>Confirmed</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8.953 -79.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>5472</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Africa</td>\n",
       "      <td>-20.156</td>\n",
       "      <td>SQ-1022948</td>\n",
       "      <td>28.581</td>\n",
       "      <td>Chicken Slice</td>\n",
       "      <td>Not confirmed</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>-20.156 28.581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "incident_level  city_id      city_name        country_name macroregion_name main_from_latitude main_incident_id main_from_longitude                        main_from_address information_status  Green  Red  Yellow  total      coordinates\n",
       "123                4258       Arequipa                Peru    Latin America            -16.392       SQ-1041094              -71.54                         Cl√≠nica Arequipa          Confirmed     23    0       9     32   -16.392 -71.54\n",
       "82                 4199           Lima                Peru    Latin America            -12.057       SQ-1040967             -77.001                          Pje. Plut√≥n 160          Confirmed     28    0       3     31  -12.057 -77.001\n",
       "146                4300      Cape Town        South Africa           Africa            -33.991       SQ-1042637              18.567           Sokhanyo Public Primary School          Confirmed     20    1       4     25   -33.991 18.567\n",
       "121                4257       Trujillo                Peru    Latin America             -8.124       SQ-1036866             -79.038                          Mi Facultad Bar          Confirmed     12    0       6     18   -8.124 -79.038\n",
       "108                4243   Barranquilla            Colombia    Latin America             10.983       SQ-1042334             -74.784                                  Baranoa          Confirmed      9    0       5     14   10.983 -74.784\n",
       "212                4825  Santo Domingo  Dominican Republic    Latin America             18.482       SQ-1042097              -69.83                 C. Cam. de Juan L√≥pez 25          Confirmed     10    0       3     13    18.482 -69.83\n",
       "139                4275          Piura                Peru    Latin America             -5.187       SQ-1030971             -80.623  Hospital Regional Jos√© Cayetano Heredia          Confirmed      8    0       5     13   -5.187 -80.623\n",
       "256                5388         Lahore            Pakistan               SA             31.448       SQ-1040766              74.259                      Ayesha Girls Hostel          Confirmed      9    0       4     13    31.448 74.259\n",
       "292                5548         Panama              Panama    Latin America              8.953       SQ-1041961             -79.533                               C. 3a Este          Confirmed     11    0       1     12    8.953 -79.533\n",
       "276                5472       Bulawayo            Zimbabwe           Africa            -20.156       SQ-1022948              28.581                            Chicken Slice      Not confirmed      5    0       7     12   -20.156 28.581"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = read_bq(query)\n",
    "df_raw['near_incident_date'] = pd.to_datetime(df_raw['near_incident_date'])\n",
    "\n",
    "df_raw_agg = df_raw[df_raw['date_diff']<=14].groupby([\n",
    "    'city_id',\n",
    "    'city_name',\n",
    "    'country_name',\n",
    "    'macroregion_name',\n",
    "    'main_incident_id',\n",
    "    'main_from_latitude',\n",
    "    'main_from_longitude',\n",
    "    'main_from_address',\n",
    "    'information_status',\n",
    "    'incident_level'\n",
    "], as_index=False)[\n",
    "    'near_incident_id'\n",
    "].agg(list)\n",
    "\n",
    "df_raw_agg['count_inc'] = df_raw_agg['near_incident_id'].apply(len)\n",
    "df_raw_agg['near_incident_id'] = df_raw_agg['near_incident_id'].apply(tuple)\n",
    "\n",
    "df_raw_agg_piv = df_raw_agg.pivot_table(\n",
    "    index=[\n",
    "        'city_id', 'city_name', 'country_name', \n",
    "        'macroregion_name', 'main_from_latitude', 'main_incident_id',\n",
    "        'main_from_longitude', 'main_from_address', 'information_status', \n",
    "        ],\n",
    "    columns='incident_level',\n",
    "    values='count_inc',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "for column in df_raw_agg['incident_level'].unique():\n",
    "    df_raw_agg_piv[column] = df_raw_agg_piv[column].astype(int)\n",
    "\n",
    "total = 0\n",
    "for column in df_raw_agg['incident_level'].unique():\n",
    "    total += df_raw_agg_piv[column]\n",
    "\n",
    "df_raw_agg_piv['total'] = total\n",
    "\n",
    "df_raw_agg_piv['main_from_latitude'] = df_raw_agg_piv['main_from_latitude'].round(3).astype(str)\n",
    "df_raw_agg_piv['main_from_longitude'] = df_raw_agg_piv['main_from_longitude'].round(3).astype(str)\n",
    "\n",
    "df_raw_agg_piv['coordinates'] = df_raw_agg_piv['main_from_latitude'] + ' ' + df_raw_agg_piv['main_from_longitude']\n",
    "\n",
    "df_raw_agg_piv.sort_values('total', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "46253ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_incident_id</th>\n",
       "      <th>count_now</th>\n",
       "      <th>count_before</th>\n",
       "      <th>diff</th>\n",
       "      <th>diff_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>SQ-1002960</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>600.00</td>\n",
       "      <td>600.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>SQ-1042420</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>400.00</td>\n",
       "      <td>400.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>SQ-1039476</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>333.30</td>\n",
       "      <td>333.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>SQ-1041352</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>300.00</td>\n",
       "      <td>300.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SQ-1042344</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>300.00</td>\n",
       "      <td>300.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    main_incident_id  count_now  count_before   diff diff_perc\n",
       "112       SQ-1002960          7             1 600.00    600.0%\n",
       "169       SQ-1042420          5             1 400.00    400.0%\n",
       "208       SQ-1039476         13             3 333.30    333.3%\n",
       "260       SQ-1041352          4             1 300.00    300.0%\n",
       "7         SQ-1042344          4             1 300.00    300.0%"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw_agg_now = df_raw[df_raw['date_diff']<=14].groupby([\n",
    "    'city_id',\n",
    "    'city_name',\n",
    "    'country_name',\n",
    "    'macroregion_name',\n",
    "    'main_incident_id',\n",
    "    'main_from_latitude',\n",
    "    'main_from_longitude',\n",
    "    'main_from_address'\n",
    "], as_index=False)[\n",
    "    'near_incident_id'\n",
    "].agg(list)\n",
    "\n",
    "df_raw_agg_now['count_inc'] = df_raw_agg_now['near_incident_id'].apply(len)\n",
    "df_raw_agg_now['near_incident_id'] = df_raw_agg_now['near_incident_id'].apply(tuple)\n",
    "\n",
    "df_raw_agg_before = df_raw[df_raw['date_diff']>14].groupby([\n",
    "    'city_id',\n",
    "    'city_name',\n",
    "    'country_name',\n",
    "    'macroregion_name',\n",
    "    'main_incident_id',\n",
    "    'main_from_latitude',\n",
    "    'main_from_longitude',\n",
    "    'main_from_address'\n",
    "], as_index=False)[\n",
    "    'near_incident_id'\n",
    "].agg(list)\n",
    "\n",
    "df_raw_agg_before['count_inc'] = df_raw_agg_before['near_incident_id'].apply(len).astype(int)\n",
    "df_raw_agg_before['near_incident_id'] = df_raw_agg_before['near_incident_id'].apply(tuple)\n",
    "\n",
    "df_check_diff = df_raw_agg_now[['main_incident_id', 'count_inc']].merge(df_raw_agg_before[['main_incident_id', 'count_inc']], how='left', on='main_incident_id').rename(columns={\n",
    "    'count_inc_x':'count_now',\n",
    "    'count_inc_y':'count_before'  \n",
    "})\n",
    "\n",
    "df_check_diff['count_before'] = df_check_diff['count_before'].fillna(0).astype(int)\n",
    "\n",
    "df_check_diff['diff'] = ((df_check_diff['count_now'] - df_check_diff['count_before'])/df_check_diff['count_before'] * 100).round(1)\n",
    "df_check_diff['diff_perc'] = ((df_check_diff['count_now'] - df_check_diff['count_before'])/df_check_diff['count_before'] * 100).round(1).astype(str) + '%'\n",
    "\n",
    "\n",
    "df_check_diff.query(\"diff_perc != 'inf%'\").sort_values('diff', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "82aed046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent successfully\n",
      "File Nearby incidents raw file.csv saved locally\n",
      "CSV-File successfully has been sent to Telegram.\n",
      "Local file Nearby incidents raw file.csv was deleted.\n",
      "File Nearby incidents aggregated file.csv saved locally\n",
      "CSV-File successfully has been sent to Telegram.\n",
      "Local file Nearby incidents aggregated file.csv was deleted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def send_telegram_text(\n",
    "    report_text: str, \n",
    "    chat_id: str, \n",
    "    token: str\n",
    ") -> bool:\n",
    "\n",
    "    try:\n",
    "        url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
    "        payload = {\n",
    "            'chat_id': chat_id,\n",
    "            'text': report_text,\n",
    "            'parse_mode': 'Markdown' \n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, data=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Sent successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error while sending. Code: {response.status_code}, Response: {response.text}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"There is an error: {e}\")\n",
    "        return False\n",
    "\n",
    "def send_telegram_csv(\n",
    "    df: pd.DataFrame, \n",
    "    file_name,\n",
    "    chat_id, \n",
    "    token\n",
    ") -> bool:\n",
    "\n",
    "    \n",
    "    file_path = f'./{file_name}'\n",
    "    success = False\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        print(f\"File {file_name} saved locally\")\n",
    "\n",
    "        document_url = f\"https://api.telegram.org/bot{token}/sendDocument\"\n",
    "        \n",
    "        with open(file_path, 'rb') as f:\n",
    "            document_payload = {\n",
    "                'chat_id': chat_id\n",
    "            }\n",
    "            document_files = {\n",
    "                'document': (file_name, f, 'text/csv')\n",
    "            }\n",
    "            \n",
    "            document_response = requests.post(\n",
    "                document_url, \n",
    "                data=document_payload, \n",
    "                files=document_files\n",
    "            )\n",
    "\n",
    "        if document_response.status_code == 200:\n",
    "            print(\"CSV-File successfully has been sent to Telegram.\")\n",
    "            success = True\n",
    "        else:\n",
    "            print(f\"Error while sending. Code: {document_response.status_code}, Response: {document_response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"There is an error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Local file {file_name} was deleted.\")\n",
    "            \n",
    "        return success\n",
    "\n",
    "# df_raw = read_bq(query)\n",
    "\n",
    "TELEGRAM_BOT_TOKEN = \"8076954277:AAG34ytld2GrnUqh_Lgz2ATWoYADuBQ-e64\"\n",
    "TELEGRAM_CHAT_ID = \"-1003078511881\"\n",
    "\n",
    "min_date = df_raw[df_raw['date_diff']<=15]['near_incident_date'].min()\n",
    "max_date = df_raw[df_raw['date_diff']<=15]['near_incident_date'].max()\n",
    "start_date = str(min_date.day) + ' ' +  min_date.strftime('%b')\n",
    "end_date = str(max_date.day) + ' ' +  max_date.strftime('%b') + ' ' + str(df_raw[df_raw['date_diff']<=10]['near_incident_date'].max().year)\n",
    "start_date_dig = df_raw[df_raw['date_diff']>15]['near_incident_date'].min().strftime('%d.%m')\n",
    "end_date_dig = df_raw[df_raw['date_diff']>15]['near_incident_date'].max().strftime('%d.%m')\n",
    "\n",
    "information_status = ['Confirmed', 'Not confirmed']\n",
    "top_5 = df_raw_agg_piv.sort_values('total', ascending=False)['main_incident_id'].unique()[:5]\n",
    "report_table = \"\"\n",
    "\n",
    "\n",
    "for index, redmine in enumerate(top_5, 1):\n",
    "\n",
    "    lat, lon = df_raw_agg_piv[(df_raw_agg_piv['main_incident_id'] == redmine)]['main_from_latitude'].unique()[0], df_raw_agg_piv[(df_raw_agg_piv['main_incident_id'] == redmine)]['main_from_longitude'].unique()[0]\n",
    "    region = df_raw_agg_piv[(df_raw_agg_piv['main_incident_id'] == redmine)][['city_id', 'city_name', 'country_name', 'main_from_address']].iloc[0].to_list()\n",
    "    city_name, city_id, country_name, address = region[1], region[0], region[2], region[3]\n",
    "    change_20d = df_check_diff[df_check_diff['main_incident_id']==redmine]['diff'].item()\n",
    "\n",
    "\n",
    "    if change_20d < 0:\n",
    "        report_table += f'{index}. *{city_name}* (`{city_id}`), *{country_name}*   *(‚Üì {change_20d}% vs. {start_date_dig}-{end_date_dig}):*\\n'\n",
    "    else:\n",
    "        report_table += f'{index}. *{city_name}* (`{city_id}`), *{country_name}*   *(‚Üë {change_20d}% vs. {start_date_dig}-{end_date_dig}):*\\n'\n",
    "\n",
    "    df_flat = df_raw_agg_piv[(df_raw_agg_piv['main_incident_id'] == redmine)].copy()\n",
    "    levels = df_raw_agg[df_raw_agg['main_incident_id']==redmine]['incident_level'].unique()\n",
    "    total = 0\n",
    "\n",
    "    for column in levels:\n",
    "        total += df_raw_agg_piv[column]\n",
    "\n",
    "    df_flat['total_levels'] = total\n",
    "    new_columns = list(levels)\n",
    "    new_columns.append('total_levels')\n",
    "    df_summary = df_flat[new_columns]\n",
    "\n",
    "    for status in information_status:\n",
    "        incident_cnt = df_raw_agg_piv[(df_raw_agg_piv['main_incident_id'] == redmine)&(df_raw_agg_piv['information_status'] == status)]['total'].sum()\n",
    "        report_table += f\"   - *{incident_cnt}* {str.lower(status)} incidents\\n\"\n",
    "\n",
    "    report_table += f'   - Levels:\\n'\n",
    "\n",
    "    for column in levels:\n",
    "        share =(df_summary[column].sum() / df_summary['total_levels'].sum()*100).round(1)\n",
    "        absolute = df_summary[column].sum()\n",
    "        report_table += f'          *{column} - {share}% ({absolute})*\\n'\n",
    "    report_table += f\"   - Location: `{lat}`, `{lon}`\\n\"\n",
    "    report_table += f\"   - Address: __[{address}](https://geo-gis.console3.com/#mapHash=5.94/{lat}/{lon}&mapSettings=layers//filters/%7B%7D/baseMap//h3/)__\\n\\n\"\n",
    "\n",
    "\n",
    "message_template = f\"\"\"\n",
    "*üö® DAILY REPORT: DANGER ZONES üö®* \n",
    "\n",
    "Have prepared a list of the locations with the highest number of incidents that occurred within *1 km*. These are our priority **\"hot spots\"** for immediate review.\n",
    "\n",
    "Period: *{start_date} - {end_date}*\n",
    "\n",
    "*üìç Top 5 Danger Locations:*\n",
    "\n",
    "{report_table}\n",
    "Please follow [the link](https://geo-gis.console3.com/) and indicate the potentially danger areas\n",
    "\n",
    "*üìä Raw Data Access:*\n",
    "The full dataset for all incidents is available via the CSV file below\n",
    "\n",
    "*Thank you for your attention!*\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    send_telegram_text(message_template, TELEGRAM_CHAT_ID, TELEGRAM_BOT_TOKEN)\n",
    "    send_telegram_csv(df_raw[['city_id', 'city_name', 'country_name', 'macroregion_name', 'main_from_latitude', 'main_from_longitude', \n",
    "        'main_from_address', 'near_incident_id', 'near_incident_date', 'near_from_latitude', 'near_from_longitude', 'near_from_address', \n",
    "        'information_status', 'incident_level', 'incident_type', 'distance']]\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .rename(columns={'distance':'distance_meters'})\\\n",
    "        .sort_values(['city_id', 'distance_meters']), 'Nearby incidents raw file.csv', TELEGRAM_CHAT_ID, TELEGRAM_BOT_TOKEN)\n",
    "    send_telegram_csv(df_raw_agg_piv.sort_values('total', ascending=False).reset_index(drop=True), 'Nearby incidents aggregated file.csv', TELEGRAM_CHAT_ID, TELEGRAM_BOT_TOKEN)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
