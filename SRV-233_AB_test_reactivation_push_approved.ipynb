{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Data analysis / Data processing\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "from datetime import time, timedelta, datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "# Maths & Stats\n",
    "import math \n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "import AB_library\n",
    "from ambrosia.designer import Designer\n",
    "from ambrosia.tester import Tester\n",
    "\n",
    "# System library\n",
    "import os\n",
    "import ipywidgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# from itables import init_notebook_mode\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "import openpyxl\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "\n",
    "# Data connection\n",
    "from google.cloud import bigquery\n",
    "bigquery_client = bigquery.Client(project='analytics-dev-333113')\n",
    "\n",
    "\n",
    "# Useful functions\n",
    "def cycle_sql(start, end, query, weeks=False):\n",
    "    \"\"\"\n",
    "    You have to use {date} in your script to add cycle date into this backets\n",
    "    \"\"\"\n",
    "    date_start = datetime.strptime(start, '%Y-%m-%d')\n",
    "    date_end = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "    if weeks == False:\n",
    "        daterange = [(date_start + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days)+1)]\n",
    "    else:\n",
    "        daterange = [(date_start + timedelta(weeks=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days//7)+1)] # weeks dividing days by 7\n",
    "\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for date in daterange:\n",
    "        counter+=1\n",
    "        print(f\"{counter}) Uploading - {date}:\", datetime.today().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        script = query.format(date = date)\n",
    "        df_cycle = bigquery_client.query(script).to_dataframe()\n",
    "        if df_cycle.empty == True:\n",
    "            print('Dataframe is empty')\n",
    "        total_df = pd.concat([df_cycle, total_df])\n",
    "    return total_df\n",
    "\n",
    "def read_bq(query, project='analytics-dev-333113'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    query_job = client.query(query)\n",
    "    result_df = query_job.to_dataframe()\n",
    "    return result_df\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(\n",
    "        html_str.replace('table','table style=\"display:inline\"'), \n",
    "        raw=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ztest_proportion(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    ab_group_col: str,\n",
    "    pairs_list: List[Tuple[int, int]] = [(0, 1)],\n",
    "    corrected_ci: float = 0.95,\n",
    "    flag_notation: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Perform proportion tests between two groups.\"\"\"\n",
    "    res_table = pd.DataFrame()\n",
    "    tail = (1 + corrected_ci) / 2\n",
    "    for pair in pairs_list:\n",
    "        num0 = df[df[ab_group_col] == pair[0]][metric_col].sum()\n",
    "        denom0 = df[df[ab_group_col] == pair[0]][metric_col].count()\n",
    "        num1 = df[df[ab_group_col] == pair[1]][metric_col].sum()\n",
    "        denom1 = df[df[ab_group_col] == pair[1]][metric_col].count()\n",
    "        p0 = num0 / denom0\n",
    "        p1 = num1 / denom1\n",
    "        std0 = df[df[ab_group_col] == pair[0]][metric_col].std()\n",
    "        std1 = df[df[ab_group_col] == pair[1]][metric_col].std()\n",
    "        r = test_proportions_2indep(\n",
    "            num0, denom0,\n",
    "            num1, denom1,\n",
    "            value=0,\n",
    "            method='wald',\n",
    "            compare='diff',\n",
    "            alternative='two-sided',\n",
    "            return_results = True\n",
    "        )\n",
    "        se = np.sqrt(r.variance)\n",
    "        delta = p1 - p0\n",
    "        delta_per = (p1 / p0 - 1) * 100\n",
    "        lb = delta - stats.norm.ppf(tail) * se\n",
    "        ub = delta + stats.norm.ppf(tail) * se\n",
    "        lb_per = lb * 100 / p0\n",
    "        ub_per = ub * 100 / p0\n",
    "        \n",
    "        if flag_notation == True:\n",
    "            print(f'\\nComparison between groups: {pair[0]} and {pair[1]}')\n",
    "            print(f'statistic: {r.statistic}, pvalue: {r.pvalue}')\n",
    "            print(f'delta = {delta}')\n",
    "            print(f'delta,% = {delta_per}%')\n",
    "            print(f'Confidence interval for delta: ({lb}, {ub})')\n",
    "            print(f'Confidence interval for delta, %: ({lb_per}, {ub_per})')\n",
    "\n",
    "        result = pd.DataFrame(\n",
    "            np.array([metric_col, denom0, denom1, pair[0], pair[1], r.statistic, r.pvalue, p0, p1, delta, delta_per, lb, ub, lb_per, ub_per]).reshape(1, -1),\n",
    "            columns=['metric_name', \n",
    "                     'group0_sample_size', \n",
    "                     'group1_sample_size', \n",
    "                     'group0', \n",
    "                     'group1', \n",
    "                     'statistic', \n",
    "                     'pvalue', \n",
    "                     'mean0', \n",
    "                     'mean1', \n",
    "                     'diff_mean', \n",
    "                     'diff_mean_%', \n",
    "                     'lower_boundary', \n",
    "                     'upper_boundary', \n",
    "                     'lower_boundary_%', \n",
    "                     'upper_boundary_%',]\n",
    "        )\n",
    "        res_table = pd.concat([res_table, result])\n",
    "\n",
    "        for column in res_table.columns[5:]:\n",
    "            res_table[column] = res_table[column].astype(float)\n",
    "        \n",
    "    return res_table\n",
    "\n",
    "def ttest(\n",
    "    df: pd.DataFrame,\n",
    "    metric_col: str,\n",
    "    ab_group_col: str,\n",
    "    pairs_list: List[Tuple[int, int]] = [(0, 1)],\n",
    "    corrected_ci: float = 0.95,\n",
    "    flag_notation: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Perform t-tests between two groups.\"\"\"\n",
    "    res_table = pd.DataFrame()\n",
    "    tail = (1 + corrected_ci) / 2\n",
    "    for pair in pairs_list:\n",
    "        sample0 = df.loc[df[ab_group_col] == pair[0], metric_col]\n",
    "        sample1 = df.loc[df[ab_group_col] == pair[1], metric_col]\n",
    "        m0 = sample0.mean()\n",
    "        m1 = sample1.mean()\n",
    "        v0 = sample0.std()**2\n",
    "        v1 = sample1.std()**2\n",
    "        n0 = len(sample0)\n",
    "        n1 = len(sample1)\n",
    "        t, pvalue, df_ = ws.ttest_ind(\n",
    "            sample0,\n",
    "            sample1,\n",
    "            alternative='two-sided',\n",
    "            usevar='unequal'\n",
    "        )\n",
    "        se = np.sqrt(v0 / n0 + v1 / n1)\n",
    "        delta = m1 - m0\n",
    "        delta_per = (m1 / m0 - 1) * 100\n",
    "        lb = delta - stats.t.ppf(tail, df_) * se\n",
    "        ub = delta + stats.t.ppf(tail, df_) * se\n",
    "        lb_per = lb * 100 / m0\n",
    "        ub_per = ub * 100 / m0\n",
    "        \n",
    "        if flag_notation == True:\n",
    "            print(f'\\nComparison between groups: {pair[0]} and {pair[1]}')\n",
    "            print(f't-statistic: {t}, pvalue: {pvalue}, df: {df_}')\n",
    "            print(f'delta = {delta}')\n",
    "            print(f'delta,% = {delta_per}%')\n",
    "            print(f'Confidence interval for delta: ({lb}, {ub})')\n",
    "            print(f'Confidence interval for delta, %: ({lb_per}, {ub_per})')\n",
    "\n",
    "        result = pd.DataFrame(\n",
    "            np.array([metric_col, n0, n1, pair[0], pair[1], t, \n",
    "            # df_, \n",
    "            pvalue, m0, m1, delta, delta_per, lb, ub, lb_per, ub_per]).reshape(1, -1),\n",
    "            columns=['metric_name', \n",
    "                     'group0_sample_size', \n",
    "                     'group1_sample_size',\n",
    "                     'group0', \n",
    "                     'group1', \n",
    "                     'statistic', \n",
    "                    #  'df', \n",
    "                     'pvalue', \n",
    "                     'mean0', \n",
    "                     'mean1', \n",
    "                     'diff_mean', \n",
    "                     'diff_mean_%', \n",
    "                     'lower_boundary', \n",
    "                     'upper_boundary', \n",
    "                     'lower_boundary_%', \n",
    "                     'upper_boundary_%']\n",
    "        )\n",
    "        res_table = pd.concat([res_table, result])\n",
    "    \n",
    "    for column in res_table.columns[5:]:\n",
    "        res_table[column] = res_table[column].astype(float)\n",
    "\n",
    "    return res_table\n",
    "\n",
    "def method_benjamini_hochberg(\n",
    "    pvalues: np.ndarray,\n",
    "    alpha: float = 0.05\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Apply the Benjamini-Hochberg procedure for multiple hypothesis testing.\"\"\"\n",
    "    m = len(pvalues)\n",
    "    array_alpha = np.arange(1, m + 1) * alpha / m\n",
    "    sorted_pvalue_indexes = np.argsort(pvalues)\n",
    "    res = np.zeros(m)\n",
    "    for idx, pvalue_index in enumerate(sorted_pvalue_indexes):\n",
    "        pvalue = pvalues[pvalue_index]\n",
    "        alpha_ = array_alpha[idx]\n",
    "        if pvalue <= alpha_:\n",
    "            res[pvalue_index] = 1\n",
    "        else:\n",
    "            break\n",
    "    return res.astype(int)\n",
    "\n",
    "# Shapiro-Wilk test & Distributions\n",
    "def check_normality(df, group_column, value_column):\n",
    "    groups = df[group_column].unique()\n",
    "\n",
    "    for group in groups:\n",
    "        group_data = df[df[group_column] == group][value_column].dropna() \n",
    "        stat, p = stats.shapiro(group_data)\n",
    "        print(f'Group {group}: W={stat:.4f}, p-value={p:.4f}')\n",
    "        if p > 0.05:\n",
    "            print(f'Group {group}, Metric: {value_column}: Data is normal distributed')\n",
    "        else:\n",
    "            print(f'Group {group}, Metric: {value_column}: Data is not normal distributed')\n",
    "\n",
    "def plot_distribution(df, group_column, value_column):\n",
    "\n",
    "    groups = df[group_column].unique()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10), gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "\n",
    "    sns.histplot(data=df, x=value_column, hue=group_column, kde=True, bins=30, alpha=0.4, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Graph + KDE\")\n",
    "    axes[0, 0].set_xlabel(value_column)\n",
    "    axes[0, 0].set_ylabel(\"Frequence\")\n",
    "\n",
    "    sns.boxplot(data=df, x=group_column, y=value_column, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Boxplot grouped\")\n",
    "    axes[0, 1].set_xlabel(group_column)\n",
    "    axes[0, 1].set_ylabel(value_column)\n",
    "\n",
    "    sns.histplot(df[df[group_column] == groups[0]][value_column], bins=30, kde=True, color='blue', alpha=0.5, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Hist for the {groups[0]}')\n",
    "    axes[1, 0].set_xlabel(value_column)\n",
    "    axes[1, 0].set_ylabel(\"frequence\")\n",
    "\n",
    "    sns.histplot(df[df[group_column] == groups[1]][value_column], bins=30, kde=True, color='orange', alpha=0.5, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Hist for the {groups[1]}')\n",
    "    axes[1, 1].set_xlabel(value_column)\n",
    "    axes[1, 1].set_ylabel(\"Frequence\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Levene's & Bartlet's test\n",
    "def levene(df, indicator, metric):\n",
    "    w_stats, p_value = st.levene(\n",
    "        df[df['group_name'] == 0][indicator], \n",
    "        df[df['group_name'] == 1][indicator],\n",
    "                            center=metric)\n",
    "    \n",
    "    alpha = 0.05\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"Variance are from the same population on {metric}\")\n",
    "    else:\n",
    "        print(f\"Variance are from the different population on {metric}\")\n",
    "    \n",
    "# Cohen's D\n",
    "def cohens_d(df, metric):\n",
    "    group1 = df[df['group_name']==1][metric]\n",
    "    group2 = df[df['group_name']==0][metric]\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "     \n",
    "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1 ** 2 + (n2 - 1) * std2 ** 2) / (n1 + n2 - 2))\n",
    "     \n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "     \n",
    "    # if d <= 0.3:\n",
    "    #     print(f'Small effect: d ≈ 0-0.3 ({d:.3f})')\n",
    "    # elif 0.31 <= d <= 0.8:\n",
    "    #     print(f'Medium effect: d ≈ 0.3-0.8 ({d:.3f})')\n",
    "    # elif 0.81 <= d <= 1:\n",
    "    #     print(f'Large effect: d ≈ 0.8-1 ({d:.3f})')\n",
    "\n",
    "    return d\n",
    "\n",
    "# SRM\n",
    "def srm(df):\n",
    "    srm_df = pd.DataFrame()\n",
    "\n",
    "    for city in df['city_name'].unique():\n",
    "        \n",
    "        observed = [\n",
    "            (df.query(f'group_name == 0 and city_name == \"{city}\"')['user_id'].count()), \n",
    "            (df.query(f'group_name == 1 and city_name == \"{city}\"')['user_id'].count())\n",
    "            ]\n",
    "\n",
    "        total_traffic = sum(observed)\n",
    "\n",
    "        expected = [total_traffic/2, total_traffic/2]\n",
    "\n",
    "        chi = st.chisquare(observed, f_exp = expected)\n",
    "\n",
    "        if chi[1] < 0.01:\n",
    "            conclusion = \"Sample ratio mismatch (SRM) may be present\"\n",
    "        else:\n",
    "            conclusion = \"Sample ratio mismatch (SRM) probably not present\"\n",
    "            print(f\"{city}, {chi[1]}\")\n",
    "\n",
    "        \n",
    "        new_srm_df = pd.DataFrame(\n",
    "            [[city, observed, total_traffic, expected, round(chi[1], 3), conclusion]], \n",
    "            columns=['city_name',  'sample_sizes', 'total_size', 'expected_sizes', 'chi_value', 'conclusion']\n",
    "            )\n",
    "\n",
    "        srm_df = pd.concat([srm_df, new_srm_df]).sort_values(['city_name', 'total_size'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return srm_df\n",
    "\n",
    "# Calcualting the significance by cities\n",
    "def calcualate_result(df_cr, df_abs):\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for city in df_cr['city_name'].unique():\n",
    "\n",
    "        absolute_values_keys_result = df_abs[df_abs['city_name']==f'{city}'].copy()\n",
    "\n",
    "        cr_df = ztest_proportion(df_cr[df_cr['city_name']==f'{city}'], 'has_ride', 'group_name')\n",
    "        cr_df['metric'] = 'Conversion'\n",
    "        cr_df['cohen_d'] = cohens_d(df_cr[df_cr['city_name']==f'{city}'], 'has_ride')\n",
    "\n",
    "        rides_df = ttest(absolute_values_keys_result, 'rides', 'group_name')\n",
    "        rides_df['metric'] = 'Quantitive'\n",
    "        rides_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'rides')\n",
    "\n",
    "        gmv_df = ttest(absolute_values_keys_result, 'gmv', 'group_name')\n",
    "        gmv_df['metric'] = 'Quantitive'\n",
    "        gmv_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'gmv')\n",
    "\n",
    "        orders_df = ttest(absolute_values_keys_result, 'orders', 'group_name')\n",
    "        orders_df['metric'] = 'Quantitive'\n",
    "        orders_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'orders')\n",
    "\n",
    "        df_total = pd.concat([cr_df, rides_df, gmv_df, orders_df])\n",
    "\n",
    "        df_total['region'] = city\n",
    "        df_total['segment'] = 'By city'\n",
    "        df_total['significance'] = (df_total['pvalue']<0.05)*1\n",
    "        df_total['corrected_pvalue'] = method_benjamini_hochberg(df_total['pvalue'].values)\n",
    "\n",
    "        df_results = pd.concat([df_results, df_total])\n",
    "\n",
    "    total_cr_df = ztest_proportion(df_cr, 'has_ride', 'group_name')\n",
    "    total_cr_df['metric'] = 'Conversion'\n",
    "    total_cr_df['cohen_d'] = cohens_d(df_cr, 'has_ride')\n",
    "\n",
    "    total_rides_df = ttest(df_abs, 'rides', 'group_name')\n",
    "    total_rides_df['metric'] = 'Quantitive'\n",
    "    total_rides_df['cohen_d'] = cohens_d(df_abs, 'rides')\n",
    "\n",
    "    total_gmv_df = ttest(df_abs, 'gmv', 'group_name')\n",
    "    total_gmv_df['metric'] = 'Quantitive'\n",
    "    total_gmv_df['cohen_d'] = cohens_d(df_abs, 'gmv')\n",
    "\n",
    "    total_orders_df = ttest(df_abs, 'orders', 'group_name')\n",
    "    total_orders_df['metric'] = 'Quantitive'\n",
    "    total_orders_df['cohen_d'] = cohens_d(df_abs, 'orders')\n",
    "\n",
    "\n",
    "    total_total_df = pd.concat([total_cr_df, total_rides_df, total_gmv_df, total_orders_df])\n",
    "    total_total_df['region'] = 'All'\n",
    "    total_total_df['segment'] = 'Total'\n",
    "    total_total_df['significance'] = (df_total['pvalue']<0.05)*1\n",
    "    total_total_df['corrected_pvalue'] = method_benjamini_hochberg(df_total['pvalue'].values)\n",
    "\n",
    "    df_results = pd.concat([df_results, total_total_df])\n",
    "\n",
    "    df_results\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print('1')\n",
    "df_to_approve_ride = read_bq(\"\"\"\n",
    "   WITH liveness AS (SELECT user_id,\n",
    "                         os_name,\n",
    "                         event_dt_part,\n",
    "                         city_id,\n",
    "                         country_id,\n",
    "                         city_name,\n",
    "                         country_name,\n",
    "                         fulfilled_flow,\n",
    "                         MIN(IF(name = 'client.verification_flow_result_status.show' AND\n",
    "                                LOWER(status) = 'approve',\n",
    "                                client_time,\n",
    "                                NULL)) AS status_result\n",
    "                  FROM (SELECT user_id,\n",
    "                               name,\n",
    "                               os_name,\n",
    "                               event_dt_part,\n",
    "                               TIMESTAMP_MILLIS(client_time)                                           AS client_time,\n",
    "                               t1.city_id,\n",
    "                               t2.city_name,\n",
    "                               t2.country_id,\n",
    "                               t2.country_name,\n",
    "                               IF(LAG(JSON_EXTRACT_SCALAR(payload, '$.verification_flow'))\n",
    "                                      OVER (PARTITION BY user_id, event_dt_part ORDER BY client_time) IS NULL,\n",
    "                                  JSON_EXTRACT_SCALAR(payload, '$.verification_flow'),\n",
    "                                  LAG(JSON_EXTRACT_SCALAR(payload, '$.verification_flow'))\n",
    "                                      OVER (PARTITION BY user_id, event_dt_part ORDER BY client_time)) AS fulfilled_flow,\n",
    "                               JSON_EXTRACT_SCALAR(payload, '$.status')                                AS status\n",
    "                        FROM indriver-e6e40.ods_event_tracker.event t1\n",
    "                                 JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                                      ON\n",
    "                                          t1.city_id = t2.city_id\n",
    "                        WHERE 1 = 1\n",
    "                          AND name IN (\n",
    "                                       'client.verification_start.show',\n",
    "                                       'client.verification_flow_result_status.show'\n",
    "                            )\n",
    "                          AND event_dt_part >= '2025-02-01'\n",
    "                          AND t2.city_id IN\n",
    "                              (4263, 4267, 4243, 4545, 4540, 4197, 4530, 5568, 4255, 4559, 4300, 4227, 19943, 4261,\n",
    "                               5573, 4266, 4196, 4376, 4154, 798, 4225, 4198, 4385, 4271, 4374, 4299, 5368, 4229, 4199,\n",
    "                               4524, 4242, 4143, 4155, 4517, 5589, 5548, 4755, 4397, 4226, 4269, 4404, 5600, 4373, 4375,\n",
    "                               4153, 4231, 5535, 4200, 5528, 4234, 4825, 4142, 5536, 4264, 4549, 4228, 5291, 4257,\n",
    "                               4516))\n",
    "                  GROUP BY 1, 2, 3, 4, 5, 6, 7, 8),\n",
    "     rides AS (SELECT order_uuid,\n",
    "                      user_id    AS pass_id,\n",
    "                      driver_id,\n",
    "                      city_id    AS order_city_id,\n",
    "                      country_id AS order_country_id,\n",
    "                      status_order,\n",
    "                      order_timestamp,\n",
    "                      at_pickup_dttm,\n",
    "                      departed_pickup_dttm,\n",
    "                      at_destination_dttm,\n",
    "                      departed_destination_dttm,\n",
    "                      driveraccept_timestamp,\n",
    "                      driverarrived_timestamp,\n",
    "                      driverstarttheride_timestamp,\n",
    "                      driverdone_timestamp,\n",
    "                      clientdone_timestamp,\n",
    "                      clientcancel_timestamp,\n",
    "                      drivercancel_timestamp,\n",
    "                      user_reg_date,\n",
    "                      driver_reg_date,\n",
    "                      stage,\n",
    "                      created_date_order_part,\n",
    "                      duration_in_seconds\n",
    "               FROM indriver-e6e40.imart.incity_detail_new_order\n",
    "               WHERE created_date_order_part >= '2025-02-01'\n",
    "                 AND city_id IN\n",
    "                     (4263, 4267, 4243, 4545, 4540, 4197, 4530, 5568, 4255, 4559, 4300, 4227, 19943, 4261, 5573, 4266,\n",
    "                      4196, 4376, 4154, 798, 4225, 4198, 4385, 4271, 4374, 4299, 5368, 4229, 4199, 4524, 4242, 4143,\n",
    "                      4155, 4517, 5589, 5548, 4755, 4397, 4226, 4269, 4404, 5600, 4373, 4375, 4153, 4231, 5535, 4200,\n",
    "                      5528, 4234, 4825, 4142, 5536, 4264, 4549, 4228, 5291, 4257, 4516)\n",
    "                 AND driveraccept_timestamp IS NOT NULL\n",
    "                 AND (clientcancel_timestamp IS NULL\n",
    "                   AND drivercancel_timestamp IS NULL))\n",
    "    SELECT t1.user_id,\n",
    "        t1.os_name,\n",
    "        t1.city_id,\n",
    "        t1.country_id,\n",
    "        t1.city_name,\n",
    "        t1.country_name,\n",
    "        t1.status_result,\n",
    "        t2.order_timestamp,\n",
    "        status_order\n",
    "    FROM liveness t1\n",
    "            LEFT JOIN rides t2\n",
    "                    ON t1.user_id = t2.pass_id\n",
    "                        AND t2.order_timestamp BETWEEN status_result AND DATE_ADD(status_result, INTERVAL + 1 DAY)\n",
    "    WHERE 1 = 1\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY order_timestamp ASC) = 1\n",
    "\"\"\")\n",
    "\n",
    "print('2')\n",
    "df_rides_gmv_orders = read_bq(\"\"\"\n",
    "    WITH rides AS (SELECT t1.user_id,\n",
    "                      t1.country_id,\n",
    "                      t2.country_name,\n",
    "                      t1.city_id,\n",
    "                      t2.city_name,\n",
    "                      AVG(orders_count)  AS orders,\n",
    "                      AVG(rides_count)   AS rides,\n",
    "                      SUM(gmv_clean_usd) AS gmv\n",
    "               FROM indriver-bi.incity.tbl_incity_growth_metrics_detail t1\n",
    "                        JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                             ON t1.city_id = t2.city_id\n",
    "               WHERE user_type = 'pass'\n",
    "                 AND t1.city_id IN (4263, 4267, 4243, 4545, 4540, 4197, 4530, 5568, 4255, 4559, 4300, 4227, 19943,\n",
    "                                    4261, 5573, 4266, 4196, 4376, 4154, 798, 4225, 4198, 4385, 4271, 4374, 4299,\n",
    "                                    5368, 4229, 4199, 4524, 4242, 4143, 4155, 4517, 5589, 5548, 4755, 4397, 4226,\n",
    "                                    4269, 4404, 5600, 4373, 4375, 4153, 4231, 5535, 4200, 5528, 4234, 4825, 4142,\n",
    "                                    5536, 4264, 4549, 4228, 5291, 4257, 4516)\n",
    "                 AND metric_date_utc >= '2025-03-02'\n",
    "               GROUP BY t1.user_id, t1.country_id, t2.country_name, t1.city_id, t2.city_name),\n",
    "     newbies AS (SELECT DISTINCT user_id\n",
    "                 FROM indriver-bi.incity.tbl_incity_growth_metrics_detail\n",
    "                 WHERE user_type = 'pass'\n",
    "                   AND rides_count > 0\n",
    "                   AND city_id IN (4263, 4267, 4243, 4545, 4540, 4197, 4530, 5568, 4255, 4559, 4300, 4227, 19943,\n",
    "                                   4261, 5573, 4266, 4196, 4376, 4154, 798, 4225, 4198, 4385, 4271, 4374, 4299,\n",
    "                                   5368, 4229, 4199, 4524, 4242, 4143, 4155, 4517, 5589, 5548, 4755, 4397, 4226,\n",
    "                                   4269, 4404, 5600, 4373, 4375, 4153, 4231, 5535, 4200, 5528, 4234, 4825, 4142,\n",
    "                                   5536, 4264, 4549, 4228, 5291, 4257, 4516)\n",
    "                   AND metric_date_utc BETWEEN '2024-01-01' AND '2025-03-01')\n",
    "    SELECT r.*\n",
    "    FROM rides r\n",
    "    WHERE r.user_id NOT IN (SELECT user_id FROM newbies);\n",
    "    \"\"\")\n",
    "\n",
    "print('3')\n",
    "df_sample_size = read_bq(\"\"\"\n",
    "        WITH checker AS (SELECT user_id,\n",
    "                        os_name,\n",
    "                        event_dt_part,\n",
    "                        DATE_TRUNC(event_dt_part, WEEK)  AS weekly,\n",
    "                        DATE_TRUNC(event_dt_part, MONTH) AS monthly,\n",
    "                        city_id,\n",
    "                        country_id,\n",
    "                        city_name,\n",
    "                        country_name,\n",
    "                        fulfilled_flow,\n",
    "                        MIN(IF(name = 'client.verification_start.show',\n",
    "                               client_time,\n",
    "                               NULL))                    AS banner_show,\n",
    "                        MIN(IF(name = 'client.verification_start.click',\n",
    "                               client_time,\n",
    "                               NULL))                    AS banner_click,\n",
    "                        MIN(IF(name = 'client.verification_flow_result_status.show' AND\n",
    "                               LOWER(status) = 'approve',\n",
    "                               client_time,\n",
    "                               NULL))                    AS status_result\n",
    "                 FROM (SELECT user_id,\n",
    "                              name,\n",
    "                              os_name,\n",
    "                              event_dt_part,\n",
    "                              TIMESTAMP_MILLIS(client_time)                            AS client_time,\n",
    "                              t1.city_id,\n",
    "                              t2.city_name,\n",
    "                              t2.country_id,\n",
    "                              t2.country_name,\n",
    "                              IF(LAG(JSON_EXTRACT_SCALAR(payload, '$.verification_flow'))\n",
    "                                     OVER (PARTITION BY user_id ORDER BY client_time) IS NULL,\n",
    "                                 JSON_EXTRACT_SCALAR(payload, '$.verification_flow'),\n",
    "                                 LAG(JSON_EXTRACT_SCALAR(payload, '$.verification_flow'))\n",
    "                                     OVER (PARTITION BY user_id ORDER BY client_time)) AS fulfilled_flow,\n",
    "                              JSON_EXTRACT_SCALAR(payload, '$.status')                 AS status\n",
    "                       FROM indriver-e6e40.ods_event_tracker.event t1\n",
    "                                JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                                     ON\n",
    "                                         t1.city_id = t2.city_id\n",
    "                       WHERE 1 = 1\n",
    "                         AND name IN (\n",
    "                                      'client.verification_start.show',\n",
    "                                      'client.verification_flow_result_status.show',\n",
    "                                      'client.verification_start.click'\n",
    "                           )\n",
    "                         AND event_dt_part BETWEEN '2025-02-01' AND '2025-03-30'\n",
    "                         AND t2.city_id IN\n",
    "                             (4263, 4267, 4243, 4545, 4540, 4197, 4530, 5568, 4255, 4559, 4300, 4227, 19943, 4261,\n",
    "                              5573, 4266, 4196, 4376, 4154, 798, 4225, 4198, 4385, 4271, 4374, 4299, 5368, 4229, 4199,\n",
    "                              4524, 4242, 4143, 4155, 4517, 5589, 5548, 4755, 4397, 4226, 4269, 4404, 5600, 4373, 4375,\n",
    "                              4153, 4231, 5535, 4200, 5528, 4234, 4825, 4142, 5536, 4264, 4549, 4228, 5291, 4257,\n",
    "                              4516)\n",
    "                       QUALIFY ROW_NUMBER() OVER (PARTITION BY user_id, name, event_dt_part ORDER BY client_time) = 1)\n",
    "                 GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "       SELECT event_dt_part, weekly, monthly, COUNT(DISTINCT user_id) AS users\n",
    "       FROM checker\n",
    "       WHERE status_result IS NOT NULL\n",
    "       GROUP BY 1, 2, 3\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDE & Period calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Errors ($\\alpha$, $\\beta$)</th>\n",
       "      <th>(0.01; 0.1)</th>\n",
       "      <th>(0.01; 0.2)</th>\n",
       "      <th>(0.05; 0.1)</th>\n",
       "      <th>(0.05; 0.2)</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group sizes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25389</th>\n",
       "      <td>3.9%</td>\n",
       "      <td>3.4%</td>\n",
       "      <td>3.2%</td>\n",
       "      <td>2.8%</td>\n",
       "      <td>cr_to_ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147259</th>\n",
       "      <td>1.6%</td>\n",
       "      <td>1.4%</td>\n",
       "      <td>1.3%</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>cr_to_ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294518</th>\n",
       "      <td>1.1%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>cr_to_ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736298</th>\n",
       "      <td>0.7%</td>\n",
       "      <td>0.6%</td>\n",
       "      <td>0.6%</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>cr_to_ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25389</th>\n",
       "      <td>3.3%</td>\n",
       "      <td>2.9%</td>\n",
       "      <td>2.8%</td>\n",
       "      <td>2.4%</td>\n",
       "      <td>rides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147259</th>\n",
       "      <td>1.4%</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>1.2%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>rides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294518</th>\n",
       "      <td>1.0%</td>\n",
       "      <td>0.9%</td>\n",
       "      <td>0.8%</td>\n",
       "      <td>0.7%</td>\n",
       "      <td>rides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736298</th>\n",
       "      <td>0.6%</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>0.4%</td>\n",
       "      <td>rides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25389</th>\n",
       "      <td>6.4%</td>\n",
       "      <td>5.6%</td>\n",
       "      <td>5.4%</td>\n",
       "      <td>4.6%</td>\n",
       "      <td>gmv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147259</th>\n",
       "      <td>2.6%</td>\n",
       "      <td>2.3%</td>\n",
       "      <td>2.2%</td>\n",
       "      <td>1.9%</td>\n",
       "      <td>gmv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294518</th>\n",
       "      <td>1.9%</td>\n",
       "      <td>1.7%</td>\n",
       "      <td>1.6%</td>\n",
       "      <td>1.4%</td>\n",
       "      <td>gmv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736298</th>\n",
       "      <td>1.2%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>1.0%</td>\n",
       "      <td>0.9%</td>\n",
       "      <td>gmv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Errors ($\\alpha$, $\\beta$) (0.01; 0.1) (0.01; 0.2) (0.05; 0.1) (0.05; 0.2)      metric\n",
       "Group sizes                                                                           \n",
       "25389                             3.9%        3.4%        3.2%        2.8%  cr_to_ride\n",
       "147259                            1.6%        1.4%        1.3%        1.2%  cr_to_ride\n",
       "294518                            1.1%        1.0%        1.0%        0.8%  cr_to_ride\n",
       "736298                            0.7%        0.6%        0.6%        0.5%  cr_to_ride\n",
       "25389                             3.3%        2.9%        2.8%        2.4%       rides\n",
       "147259                            1.4%        1.2%        1.2%        1.0%       rides\n",
       "294518                            1.0%        0.9%        0.8%        0.7%       rides\n",
       "736298                            0.6%        0.5%        0.5%        0.4%       rides\n",
       "25389                             6.4%        5.6%        5.4%        4.6%         gmv\n",
       "147259                            2.6%        2.3%        2.2%        1.9%         gmv\n",
       "294518                            1.9%        1.7%        1.6%        1.4%         gmv\n",
       "736298                            1.2%        1.0%        1.0%        0.9%         gmv"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily = int(df_sample_size.groupby(['event_dt_part'], as_index=False)['users'].sum()['users'].mean()*0.6)\n",
    "weekly = int(df_sample_size.groupby(['weekly'], as_index=False)['users'].sum()['users'].mean()*0.6)\n",
    "monthly = int(df_sample_size.groupby(['monthly'], as_index=False)['users'].sum()['users'].mean()*0.6)\n",
    "\n",
    "effects = [1.01, 1.015, 1.05, 1.1]  # MDE in percents\n",
    "sizes = [daily, weekly, weekly*2, monthly]  # Size of each group\n",
    "first_type_errors = [0.01, 0.05]\n",
    "second_type_errors = [0.1, 0.2]\n",
    "\n",
    "df_cr_to_ride_agg = df_to_approve_ride.groupby(['user_id', 'city_name'], as_index=False)['order_timestamp'].count()\n",
    "df_cr_to_approve = df_to_approve_ride.groupby(['user_id', 'city_name'], as_index=False)['status_result'].count()\n",
    "\n",
    "design_cr_to_ride = Designer(\n",
    "        dataframe=df_cr_to_ride_agg, \n",
    "        metrics='order_timestamp'\n",
    "        )\n",
    "\n",
    "design_cr_to_ride.set_first_errors(first_type_errors)\n",
    "design_cr_to_ride.set_second_errors(second_type_errors)\n",
    "\n",
    "df_cr_to_ride_design = design_cr_to_ride.run(\n",
    "            to_design='effect', \n",
    "            method='theory', \n",
    "            effects=effects,\n",
    "            sizes=sizes\n",
    "            )\n",
    "df_cr_to_ride_design['metric'] = 'cr_to_ride'\n",
    "\n",
    "design_gmv = Designer(\n",
    "        dataframe=df_rides_gmv_orders, \n",
    "        metrics='gmv'\n",
    "        )\n",
    "\n",
    "design_gmv.set_first_errors(first_type_errors)\n",
    "design_gmv.set_second_errors(second_type_errors)\n",
    "\n",
    "df_gmv_design = design_gmv.run(\n",
    "            to_design='effect', \n",
    "            method='theory', \n",
    "            effects=effects,\n",
    "            sizes=sizes\n",
    "            )\n",
    "df_gmv_design['metric'] = 'gmv'\n",
    "\n",
    "design_rides = Designer(\n",
    "        dataframe=df_rides_gmv_orders, \n",
    "        metrics='rides'\n",
    "        )\n",
    "\n",
    "design_rides.set_first_errors(first_type_errors)\n",
    "design_rides.set_second_errors(second_type_errors)\n",
    "\n",
    "df_rides_design = design_rides.run(\n",
    "            to_design='effect', \n",
    "            method='theory', \n",
    "            effects=effects,\n",
    "            sizes=sizes\n",
    "            )\n",
    "df_rides_design['metric'] = 'rides'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.concat([df_cr_to_ride_design, df_rides_design, df_gmv_design])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the results. Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pulling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the key statistics over the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the test for significance calculating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
