{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b9812a",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f224ccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Data analysis / Data processing\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "from datetime import time, timedelta, datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "# Maths & Stats\n",
    "import math \n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.proportion import test_proportions_2indep\n",
    "import AB_library\n",
    "# from ambrosia.designer import Designer\n",
    "# from ambrosia.tester import Tester\n",
    "import expab\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "from math import ceil\n",
    "\n",
    "# System library\n",
    "import os\n",
    "import ipywidgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "%config InlineBackend.figure_format='retina'\n",
    "# from itables import init_notebook_mode\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "import openpyxl\n",
    "\n",
    "# Data connection\n",
    "from google.cloud import bigquery\n",
    "bigquery_client = bigquery.Client(project='analytics-dev-333113')\n",
    "\n",
    "\n",
    "# Useful functions\n",
    "def read_bq(query, project='analytics-dev-333113'):\n",
    "    client = bigquery.Client(project=project)\n",
    "    query_job = client.query(query)\n",
    "    result_df = query_job.to_dataframe()\n",
    "    return result_df\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(\n",
    "        html_str.replace('table','table style=\"display:inline\"'), \n",
    "        raw=True\n",
    "    )\n",
    "\n",
    "def cycle_sql(start, end, query, weeks=False):\n",
    "    \"\"\"\n",
    "    You have to use {date} in your script to add cycle date into this backets\n",
    "    \"\"\"\n",
    "    date_start = datetime.strptime(start, '%Y-%m-%d')\n",
    "    date_end = datetime.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "    if weeks == False:\n",
    "        daterange = [(date_start + timedelta(days=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days)+1)]\n",
    "    else:\n",
    "        daterange = [(date_start + timedelta(weeks=x)).strftime('%Y-%m-%d') for x in range(((date_end-date_start).days//7)+1)] # weeks dividing days by 7\n",
    "\n",
    "    total_df = pd.DataFrame()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for date in daterange:\n",
    "        counter+=1\n",
    "        print(f\"{counter}) Uploading - {date}:\", datetime.today().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        script = query.format(date = date)\n",
    "        df_cycle = bigquery_client.query(script).to_dataframe()\n",
    "        if df_cycle.empty == True:\n",
    "            print('Dataframe is empty')\n",
    "        total_df = pd.concat([df_cycle, total_df])\n",
    "    return total_df  \n",
    "\n",
    "def writing_excel(name:str, dataset1=None, dataset2=None, dataset3=None, dataset4=None):\n",
    "    with pd.ExcelWriter(f\"{name}.xlsx\") as writer:\n",
    "\n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "\n",
    "        if dataset1 is not None:\n",
    "            if dataset2 is not None:\n",
    "                if dataset3 is not None:\n",
    "                    if dataset4 is not None:\n",
    "                        dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset3.to_excel(writer, sheet_name=f\"3-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset4.to_excel(writer, sheet_name=f\"4-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                    else:\n",
    "                        dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                        dataset3.to_excel(writer, sheet_name=f\"3-{name}\", \n",
    "                                        #   index=False\n",
    "                                            )\n",
    "                else:\n",
    "                    dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                    #   index=False\n",
    "                                        )\n",
    "                    dataset2.to_excel(writer, sheet_name=f\"2-{name}\", \n",
    "                                    #   index=False\n",
    "                                        )\n",
    "            else:\n",
    "                dataset1.to_excel(writer, sheet_name=f\"1-{name}\", \n",
    "                                #   index=False\n",
    "                                    )\n",
    "\n",
    "        print('DataFrame is written to Excel File successfully.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887b1d5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3837c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_benjamini_hochberg(\n",
    "    pvalues: np.ndarray,\n",
    "    alpha: float = 0.05\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Apply the Benjamini-Hochberg procedure for multiple hypothesis testing.\"\"\"\n",
    "    m = len(pvalues)\n",
    "    array_alpha = np.arange(1, m + 1) * alpha / m\n",
    "    sorted_pvalue_indexes = np.argsort(pvalues)\n",
    "    res = np.zeros(m)\n",
    "    for idx, pvalue_index in enumerate(sorted_pvalue_indexes):\n",
    "        pvalue = pvalues[pvalue_index]\n",
    "        alpha_ = array_alpha[idx]\n",
    "        if pvalue <= alpha_:\n",
    "            res[pvalue_index] = 1\n",
    "        else:\n",
    "            break\n",
    "    return res.astype(int)\n",
    "\n",
    "# Shapiro-Wilk test & Distributions\n",
    "def check_normality(df, group_column, value_column):\n",
    "    groups = df[group_column].unique()\n",
    "\n",
    "    for group in groups:\n",
    "        group_data = df[df[group_column] == group][value_column].dropna() \n",
    "        stat, p = stats.shapiro(group_data)\n",
    "        print(f'Group {group}: W={stat:.4f}, p-value={p:.4f}')\n",
    "        if p > 0.05:\n",
    "            print(f'Group {group}, Metric: {value_column}: Data is normal distributed')\n",
    "        else:\n",
    "            print(f'Group {group}, Metric: {value_column}: Data is not normal distributed')\n",
    "\n",
    "def plot_distribution(df, group_column, value_column):\n",
    "\n",
    "    groups = df[group_column].unique()\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10), gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "\n",
    "    sns.histplot(data=df, x=value_column, hue=group_column, kde=True, bins=30, alpha=0.4, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(\"Graph + KDE\")\n",
    "    axes[0, 0].set_xlabel(value_column)\n",
    "    axes[0, 0].set_ylabel(\"Frequence\")\n",
    "\n",
    "    sns.boxplot(data=df, x=group_column, y=value_column, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"Boxplot grouped\")\n",
    "    axes[0, 1].set_xlabel(group_column)\n",
    "    axes[0, 1].set_ylabel(value_column)\n",
    "\n",
    "    sns.histplot(df[df[group_column] == groups[0]][value_column], bins=30, kde=True, color='blue', alpha=0.5, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Hist for the {groups[0]}')\n",
    "    axes[1, 0].set_xlabel(value_column)\n",
    "    axes[1, 0].set_ylabel(\"frequence\")\n",
    "\n",
    "    sns.histplot(df[df[group_column] == groups[1]][value_column], bins=30, kde=True, color='orange', alpha=0.5, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Hist for the {groups[1]}')\n",
    "    axes[1, 1].set_xlabel(value_column)\n",
    "    axes[1, 1].set_ylabel(\"Frequence\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Levene's & Bartlet's test\n",
    "def levene(df, indicator, metric):\n",
    "    w_stats, p_value = st.levene(\n",
    "        df[df['group_name'] == 0][indicator], \n",
    "        df[df['group_name'] == 1][indicator],\n",
    "                            center=metric)\n",
    "    \n",
    "    alpha = 0.05\n",
    "    \n",
    "    if p_value > alpha:\n",
    "        print(f\"Variance are from the same population on {metric}\")\n",
    "    else:\n",
    "        print(f\"Variance are from the different population on {metric}\")\n",
    "    \n",
    "# Cohen's D\n",
    "def cohens_d(df, metric):\n",
    "    group1 = df[df['group_name']==1][metric]\n",
    "    group2 = df[df['group_name']==0][metric]\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "     \n",
    "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1 ** 2 + (n2 - 1) * std2 ** 2) / (n1 + n2 - 2))\n",
    "     \n",
    "    d = (mean1 - mean2) / pooled_std\n",
    "     \n",
    "    # if d <= 0.3:\n",
    "    #     print(f'Small effect: d ≈ 0-0.3 ({d:.3f})')\n",
    "    # elif 0.31 <= d <= 0.8:\n",
    "    #     print(f'Medium effect: d ≈ 0.3-0.8 ({d:.3f})')\n",
    "    # elif 0.81 <= d <= 1:\n",
    "    #     print(f'Large effect: d ≈ 0.8-1 ({d:.3f})')\n",
    "\n",
    "    return d\n",
    "\n",
    "# SRM\n",
    "def srm(df):\n",
    "    srm_df = pd.DataFrame()\n",
    "\n",
    "    for city in df['city_name'].unique():\n",
    "        \n",
    "        observed = [\n",
    "            (df.query(f'group_name == 0 and city_name == \"{city}\"')['user_id'].count()), \n",
    "            (df.query(f'group_name == 1 and city_name == \"{city}\"')['user_id'].count())\n",
    "            ]\n",
    "\n",
    "        total_traffic = sum(observed)\n",
    "\n",
    "        expected = [total_traffic/2, total_traffic/2]\n",
    "\n",
    "        chi = st.chisquare(observed, f_exp = expected)\n",
    "\n",
    "        if chi[1] < 0.01:\n",
    "            conclusion = \"Sample ratio mismatch (SRM) may be present\"\n",
    "        else:\n",
    "            conclusion = \"Sample ratio mismatch (SRM) probably not present\"\n",
    "            print(f\"{city}, {chi[1]}\")\n",
    "\n",
    "        \n",
    "        new_srm_df = pd.DataFrame(\n",
    "            [[city, observed, total_traffic, expected, round(chi[1], 3), conclusion]], \n",
    "            columns=['city_name',  'sample_sizes', 'total_size', 'expected_sizes', 'chi_value', 'conclusion']\n",
    "            )\n",
    "\n",
    "        srm_df = pd.concat([srm_df, new_srm_df]).sort_values(['city_name', 'total_size'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return srm_df\n",
    "\n",
    "# Calcualting the significance by cities\n",
    "def calcualate_result(df_cr, df_abs):\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for city in df_cr['city_name'].unique():\n",
    "\n",
    "        absolute_values_keys_result = df_abs[df_abs['city_name']==f'{city}'].copy()\n",
    "\n",
    "        cr_df = ztest_proportion(df_cr[df_cr['city_name']==f'{city}'], 'has_ride', 'group_name')\n",
    "        cr_df['metric'] = 'Conversion'\n",
    "        cr_df['cohen_d'] = cohens_d(df_cr[df_cr['city_name']==f'{city}'], 'has_ride')\n",
    "\n",
    "        rides_df = ttest(absolute_values_keys_result, 'rides', 'group_name')\n",
    "        rides_df['metric'] = 'Quantitive'\n",
    "        rides_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'rides')\n",
    "\n",
    "        gmv_df = ttest(absolute_values_keys_result, 'gmv', 'group_name')\n",
    "        gmv_df['metric'] = 'Quantitive'\n",
    "        gmv_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'gmv')\n",
    "\n",
    "        orders_df = ttest(absolute_values_keys_result, 'orders', 'group_name')\n",
    "        orders_df['metric'] = 'Quantitive'\n",
    "        orders_df['cohen_d'] = cohens_d(absolute_values_keys_result, 'orders')\n",
    "\n",
    "        df_total = pd.concat([cr_df, rides_df, gmv_df, orders_df])\n",
    "\n",
    "        df_total['region'] = city\n",
    "        df_total['segment'] = 'By city'\n",
    "        df_total['significance'] = (df_total['pvalue']<0.05)*1\n",
    "        df_total['corrected_pvalue'] = method_benjamini_hochberg(df_total['pvalue'].values)\n",
    "\n",
    "        df_results = pd.concat([df_results, df_total])\n",
    "\n",
    "    total_cr_df = ztest_proportion(df_cr, 'has_ride', 'group_name')\n",
    "    total_cr_df['metric'] = 'Conversion'\n",
    "    total_cr_df['cohen_d'] = cohens_d(df_cr, 'has_ride')\n",
    "\n",
    "    total_rides_df = ttest(df_abs, 'rides', 'group_name')\n",
    "    total_rides_df['metric'] = 'Quantitive'\n",
    "    total_rides_df['cohen_d'] = cohens_d(df_abs, 'rides')\n",
    "\n",
    "    total_gmv_df = ttest(df_abs, 'gmv', 'group_name')\n",
    "    total_gmv_df['metric'] = 'Quantitive'\n",
    "    total_gmv_df['cohen_d'] = cohens_d(df_abs, 'gmv')\n",
    "\n",
    "    total_orders_df = ttest(df_abs, 'orders', 'group_name')\n",
    "    total_orders_df['metric'] = 'Quantitive'\n",
    "    total_orders_df['cohen_d'] = cohens_d(df_abs, 'orders')\n",
    "\n",
    "\n",
    "    total_total_df = pd.concat([total_cr_df, total_rides_df, total_gmv_df, total_orders_df])\n",
    "    total_total_df['region'] = 'All'\n",
    "    total_total_df['segment'] = 'Total'\n",
    "    total_total_df['significance'] = (df_total['pvalue']<0.05)*1\n",
    "    total_total_df['corrected_pvalue'] = method_benjamini_hochberg(df_total['pvalue'].values)\n",
    "\n",
    "    df_results = pd.concat([df_results, total_total_df])\n",
    "\n",
    "    df_results\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def sequential_wald_test(df, date_col, metric_col, group_col, user_col, alpha=0.05, beta=0.2):\n",
    "    \n",
    "    A = np.round(np.log(beta / (1 - alpha)), 2)   \n",
    "    B = np.round(np.log((1 - beta) / alpha), 2) \n",
    "    \n",
    "    df_grouped = df.groupby([date_col, group_col]).agg(\n",
    "        users=(user_col, 'nunique'), \n",
    "        conversions=(metric_col, 'sum') \n",
    "    ).reset_index()\n",
    "\n",
    "    df_grouped[\"cum_users\"] = df_grouped.groupby(group_col)[\"users\"].cumsum()\n",
    "    df_grouped[\"cum_conversions\"] = df_grouped.groupby(group_col)[\"conversions\"].cumsum()\n",
    "\n",
    "    df_A = df_grouped[df_grouped[group_col] == 0].drop(columns=[group_col]).rename(\n",
    "        columns={\"users\": \"users_A\", \"conversions\": \"conv_A\", \"cum_users\": \"cum_users_A\", \"cum_conversions\": \"cum_conv_A\"}\n",
    "    )\n",
    "    df_B = df_grouped[df_grouped[group_col] == 1].drop(columns=[group_col]).rename(\n",
    "        columns={\"users\": \"users_B\", \"conversions\": \"conv_B\", \"cum_users\": \"cum_users_B\", \"cum_conversions\": \"cum_conv_B\"}\n",
    "    )\n",
    "\n",
    "    df_merged = pd.merge(df_A, df_B, on=date_col, how=\"outer\").fillna(0)\n",
    "\n",
    "    print(\"Колонки в df_merged:\", df_merged.columns)\n",
    "\n",
    "    p_values, llr_values = [], []\n",
    "    stop_day = None\n",
    "\n",
    "    for i in range(len(df_merged)):\n",
    "        try:\n",
    "            users_A, conv_A = df_merged.loc[i, [\"cum_users_A\", \"cum_conv_A\"]]\n",
    "            users_B, conv_B = df_merged.loc[i, [\"cum_users_B\", \"cum_conv_B\"]]\n",
    "\n",
    "            p_A = conv_A / users_A if users_A > 0 else 0\n",
    "            p_B = conv_B / users_B if users_B > 0 else 0\n",
    "\n",
    "            r = test_proportions_2indep(\n",
    "                conv_A, users_A,\n",
    "                conv_B, users_B,\n",
    "                value=0,\n",
    "                method='wald',\n",
    "                compare='diff',\n",
    "                alternative='two-sided',\n",
    "                return_results=True\n",
    "            )\n",
    "\n",
    "            p_value = r.pvalue\n",
    "            p_values.append(p_value)\n",
    "\n",
    "            llr = np.log(p_B / p_A) if p_B > 0 and p_A > 0 else 0\n",
    "            llr_values.append(llr)\n",
    "\n",
    "            if llr <= A:\n",
    "                stop_day = df_merged.loc[i, date_col]\n",
    "                print(f\"On {stop_day} might be stopped: LLR={llr:.3f} <= {A:.3f} (Accept H0)\")\n",
    "                break\n",
    "            elif llr >= B:\n",
    "                stop_day = df_merged.loc[i, date_col]\n",
    "                print(f\"On {stop_day} might be stopped: LLR={llr:.3f} >= {B:.3f} (Accept H1)\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Ошибка на дне {df_merged.loc[i, date_col]}: {e}\")\n",
    "            p_values.append(np.nan)\n",
    "            llr_values.append(np.nan)\n",
    "\n",
    "    # Создаем DataFrame с результатами\n",
    "    df_results = df_merged.iloc[:len(p_values)].copy()\n",
    "    df_results[\"p_value\"] = p_values\n",
    "    df_results[\"LLR\"] = llr_values\n",
    "    df_results[\"A/B\"] = str([A, B])\n",
    "    df_results[\"alpha_threshold\"] = np.linspace(alpha, alpha / np.sqrt(len(df_results)), len(p_values))  # Коррекция alpha\n",
    "\n",
    "    # Визуализация результатов\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_results[date_col], df_results[\"p_value\"], label=\"P-value\", marker=\"o\")\n",
    "    plt.plot(df_results[date_col], df_results[\"alpha_threshold\"], label=\"Corrected Alpha\", linestyle=\"dashed\")\n",
    "    plt.axhline(y=alpha, color=\"red\", linestyle=\"--\", label=\"Standard Alpha (0.05)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"P-Value\")\n",
    "    plt.title(\"P-value daily vs. Corrected Alpha\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def calculate_criteria(df):\n",
    "    df_res_1 = pd.DataFrame()\n",
    "    indicators = ['has_ride', 'rides', 'gmv']\n",
    "\n",
    "    total_res_z = expab.ztest_proportion(df, 'has_ride', 'group_name')\n",
    "    total_res_z['city_name'] = 'all the cities together'\n",
    "    total_res_t = expab.ttest(df, 'gmv', 'group_name')\n",
    "    total_res_t['city_name'] = 'all the cities together'\n",
    "    total_res_t2 = expab.ttest(df, 'rides', 'group_name')\n",
    "    total_res_t2['city_name'] = 'all the cities together'\n",
    "\n",
    "    df_res_1 = pd.concat([df_res_1, total_res_z, total_res_t, total_res_t2])\n",
    "\n",
    "    for city in df['city_name'].unique():\n",
    "        \n",
    "        for metric in indicators:\n",
    "            if metric == 'has_ride':\n",
    "                city_df_z = expab.ztest_proportion(df.query(f\"city_name == '{city}'\"), metric, 'group_name')\n",
    "                city_df_z['city_name'] = city\n",
    "\n",
    "                df_res_1 = pd.concat([df_res_1, city_df_z])\n",
    "\n",
    "            else:\n",
    "                city_df_t = expab.ttest(df.query(f\"city_name == '{city}'\"), metric, 'group_name')\n",
    "                city_df_t['city_name'] = city\n",
    "\n",
    "                df_res_1 = pd.concat([df_res_1, city_df_t])\n",
    "\n",
    "    df_res_1['corrected_pvalue'] = expab.method_benjamini_hochberg(df_res_1['pvalue'].values)\n",
    "    df_res_1['significance'] = (df_res_1['pvalue']<0.05)*1\n",
    "\n",
    "\n",
    "    return df_res_1\n",
    "\n",
    "def calculate_numbers(df):\n",
    "\n",
    "    df_agg = df.groupby(['group_name', 'city_name'], as_index=False)[['user_id', 'has_ride', 'rides', 'orders', 'gmv']].agg(\n",
    "        {'user_id':'count', \n",
    "        'has_ride':'sum', \n",
    "        'rides':'sum', \n",
    "        'gmv':'sum'}\n",
    "        ).sort_values(['city_name', 'group_name', 'user_id'], ascending=True)\n",
    "\n",
    "    df_agg['group_name'] = df_agg['group_name'].astype(str)\n",
    "\n",
    "    df_agg['group_name'] = df_agg['group_name'].replace({'0':'Control', '1':'Treatment'})\n",
    "\n",
    "    df_agg['cr_ride_%'] = np.round(df_agg['has_ride'] / df_agg['user_id'] * 100,2)\n",
    "    df_agg['cr_ride'] = np.round(df_agg['has_ride'] / df_agg['user_id'],5)\n",
    "    df_agg['cr_ride_%'] = df_agg['cr_ride_%'].astype(str)\n",
    "    df_agg['cr_ride_%'] = df_agg['cr_ride_%'] + '%'\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c26dae",
   "metadata": {},
   "source": [
    "# Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485960c2",
   "metadata": {},
   "source": [
    "### ID & Liveness hist exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a8caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>created_dt_part</th>\n",
       "      <th>os_name</th>\n",
       "      <th>city_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>country_id</th>\n",
       "      <th>country_name</th>\n",
       "      <th>newbie_flag</th>\n",
       "      <th>filled_flow</th>\n",
       "      <th>show_dt</th>\n",
       "      <th>click_dt</th>\n",
       "      <th>approve_dt</th>\n",
       "      <th>approve_flag</th>\n",
       "      <th>not_approve_dt</th>\n",
       "      <th>not_approve_flag</th>\n",
       "      <th>order_timestamp</th>\n",
       "      <th>order_flag</th>\n",
       "      <th>gmv</th>\n",
       "      <th>rides</th>\n",
       "      <th>orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10977349</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>None</td>\n",
       "      <td>4267</td>\n",
       "      <td>Arica</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 12:11:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>69.03</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11354201</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-18</td>\n",
       "      <td>None</td>\n",
       "      <td>4200</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-18 23:33:50+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>120.70</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12499562</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>android</td>\n",
       "      <td>4200</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>1</td>\n",
       "      <td>liveness</td>\n",
       "      <td>2025-05-15 12:40:40.313000+00:00</td>\n",
       "      <td>2025-05-15 12:40:51.559000+00:00</td>\n",
       "      <td>2025-05-15 12:41:50.602000+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-15 12:41:54+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11.24</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12736336</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>None</td>\n",
       "      <td>4200</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-16 13:25:16+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>46.96</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13132543</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-18</td>\n",
       "      <td>None</td>\n",
       "      <td>4267</td>\n",
       "      <td>Arica</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  group_id created_dt_part  os_name  city_id city_name  country_id country_name  newbie_flag filled_flow                          show_dt                         click_dt                       approve_dt  approve_flag not_approve_dt  not_approve_flag           order_timestamp  order_flag    gmv  rides  orders\n",
       "0  10977349         0      2025-05-15     None     4267     Arica          25        Chile            0        None                              NaT                              NaT                              NaT             0            NaT                 0 2025-05-15 12:11:06+00:00           1  69.03     24      41\n",
       "1  11354201         0      2025-05-18     None     4200  Santiago          25        Chile            0        None                              NaT                              NaT                              NaT             0            NaT                 0 2025-05-18 23:33:50+00:00           1 120.70     17      22\n",
       "2  12499562         0      2025-05-15  android     4200  Santiago          25        Chile            1    liveness 2025-05-15 12:40:40.313000+00:00 2025-05-15 12:40:51.559000+00:00 2025-05-15 12:41:50.602000+00:00             1            NaT                 0 2025-05-15 12:41:54+00:00           1  11.24      3       5\n",
       "3  12736336         0      2025-05-16     None     4200  Santiago          25        Chile            0        None                              NaT                              NaT                              NaT             0            NaT                 0 2025-05-16 13:25:16+00:00           1  46.96      6      10\n",
       "4  13132543         0      2025-05-18     None     4267     Arica          25        Chile            0        None                              NaT                              NaT                              NaT             0            NaT                 0                       NaT           0   2.57      1       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# liveness = read_bq(\"\"\"\n",
    "# DECLARE start_dt DATE;\n",
    "# DECLARE end_dt DATE;\n",
    "# SET start_dt = '2025-03-07';\n",
    "# SET end_dt = '2025-03-24';\n",
    "# WITH old_users AS (\n",
    "#     SELECT \n",
    "#         DISTINCT user_id\n",
    "#     FROM \n",
    "#         indriver-e6e40.emart.incity_detail\n",
    "#     WHERE 1=1\n",
    "#         AND DATE(created_date_order_part) < start_dt\n",
    "#         AND city_id IN (6587,4230,5495,4272,4396)\n",
    "#     GROUP BY 1\n",
    "# ),\n",
    "# ab as (\n",
    "#     SELECT user_id,\n",
    "#            max(case\n",
    "#                    when group_id = 4535352 then 0\n",
    "#                    when group_id = 4535353 then 1\n",
    "#                end) has_treatment\n",
    "#     FROM \n",
    "#         indriver-e6e40.ss_ab_platform_mart.markup_users\n",
    "#     WHERE \n",
    "#         group_id IN (4535352, 4535353)\n",
    "#         AND test_id = 2699\n",
    "#         AND user_id NOT IN (SELECT user_id FROM old_users)\n",
    "#     GROUP BY 1 \n",
    "# ),\n",
    "# appeals AS (\n",
    "#     SELECT\n",
    "#         target_id user_id,\n",
    "#         COUNT(DISTINCT uuid) cnt_appeals\n",
    "#     FROM\n",
    "#         indriver-e6e40.ods_moderation_feed_red_pill.appeal t1\n",
    "#     LEFT JOIN\n",
    "#         indriver-e6e40.ods_ds_moderation_system_cdc.violation_review_v3 t3 ON t1.uuid = JSON_EXTRACT_SCALAR(t3.payload, '$.uuid')\n",
    "#     WHERE\n",
    "#         DATE(t1.created_at) BETWEEN start_dt AND end_dt\n",
    "#         AND DATE(t3.export_raw_dt) BETWEEN start_dt AND end_dt\n",
    "#         --AND t1.initiator_id = 1 --жалоба от пассажира водиле\n",
    "#         AND t1.initiator_id = 0 --жалоба от водилы пассажиру\n",
    "#         AND t1.city_id IN (6587,4230,5495,4272,4396)\n",
    "#         AND JSON_EXTRACT_SCALAR(t3.model, '$.result.top_category[0]') NOT IN ('CATEGORY_TEXT_NOT_RECOGNISED',\n",
    "#                                                                          'CATEGORY_LOCATION_DISPUTE',\n",
    "#                                                                          'CATEGORY_RIDE_REFUSAL',\n",
    "#                                                                          'CATEGORY_CANCELLED_BY_DRIVER_REQUEST',\n",
    "#                                                                          'CATEGORY_BARGAINING_AFTER_ACCEPT',\n",
    "#                                                                          'CATEGORY_PASSENGER_WAS_LATE',\n",
    "#                                                                          'CATEGORY_DRIVER_WAS_LATE',\n",
    "#                                                                          'CATEGORY_APP_PROBLEM',\n",
    "#                                                                          'CATEGORY_POSITIVE_REVIEW',\n",
    "#                                                                          'CATEGORY_DIFFERENT_CAR',\n",
    "#                                                                          'CATEGORY_DRIVER_REPORTED_CAR_MALFUNCTION',\n",
    "#                                                                          'CATEGORY_STRANGER_IN_CAR')\n",
    "#     GROUP BY 1\n",
    "# ),\n",
    "# reviews AS (\n",
    "#     SELECT\n",
    "#         target_id as user_id,\n",
    "#         COUNT(DISTINCT r.uuid) cnt_reviews\n",
    "#     FROM \n",
    "#         indriver-e6e40.ods_moderation_feed_red_pill.review r\n",
    "#     LEFT JOIN \n",
    "#         indriver-e6e40.ods_ds_moderation_system_cdc.violation_review_v3 src on JSON_EXTRACT_SCALAR(src.payload, '$.uuid') = r.uuid\n",
    "#     WHERE\n",
    "#         DATE(r.created_at) BETWEEN start_dt AND end_dt\n",
    "#         AND DATE(src.export_raw_dt) BETWEEN start_dt AND end_dt\n",
    "#         AND rating < 5\n",
    "#         AND r.visibility_id = 1 --жалобы от водителя на пассажира \n",
    "#         AND r.city_id IN (6587,4230,5495,4272,4396)\n",
    "#         AND JSON_EXTRACT_SCALAR(src.model, '$.result.top_category[0]') NOT IN ('CATEGORY_TEXT_NOT_RECOGNISED',\n",
    "#                                                                              'CATEGORY_DIRTY_CABIN',\n",
    "#                                                                              'CATEGORY_SUSPICIOUS_AREA', --УДАЛИТЬ?\n",
    "#                                                                              'CATEGORY_ASSAULT', --УДАЛИТЬ?\n",
    "#                                                                              'CATEGORY_BARGAINING_AFTER_ACCEPT',\n",
    "#                                                                              'CATEGORY_POSITIVE_REVIEW',\n",
    "#                                                                              'CATEGORY_DIFFERENT_CAR',\n",
    "#                                                                              'CATEGORY_PASSENGER_REPORTED_CAR_MALFUNCTION',\n",
    "#                                                                              'CATEGORY_NO_CHANGE',\n",
    "#                                                                              'CATEGORY_DANGEROUS_DRIVING',\n",
    "#                                                                              'CATEGORY_CANCELLED_BY_PASSENGER_REQUEST')\n",
    "#     GROUP BY 1\n",
    "# ),\n",
    "# orders_raw AS (\n",
    "#     SELECT \n",
    "#         DISTINCT\n",
    "#         order_uuid, \n",
    "#         user_id\n",
    "#     FROM indriver-e6e40.emart.incity_detail\n",
    "#     WHERE \n",
    "#         created_date_order_part BETWEEN start_dt AND end_dt\n",
    "#         AND city_id IN (6587,4230,5495,4272,4396)\n",
    "# ),\n",
    "# support_raw AS (\n",
    "#     SELECT\n",
    "#         DISTINCT\n",
    "#         t1.id support_id,\n",
    "#         lower(t4.order_id) order_uuid\n",
    "#     FROM \n",
    "#         indriver-e6e40.ods_customer_support.request t1\n",
    "#     JOIN \n",
    "#         indriver-bi.customer_service.tbl_customer_support_chats_just_detail t2 ON t1.id = t2.request_id\n",
    "#     JOIN\n",
    "#         dwh-storage-327422.ods_customer_support.chat_request_entry t4 ON t1.id = t4.request_id\n",
    "#     JOIN\n",
    "#         (select distinct country_id, country_name, city_id from indriver-bi.heap.vw_geo_mapping) t3 ON t2.country_name = t3.country_name\n",
    "#     WHERE\n",
    "#         DATE(t1.created_dt_part) BETWEEN start_dt AND end_dt\n",
    "#         AND t4.created_dt_part BETWEEN start_dt AND end_dt\n",
    "#         AND contact_category is not null\n",
    "#         AND contact_reason is not null\n",
    "#         AND who_contacts = 'Driver'\n",
    "#         AND contact_category IN ('Complaints against Passenger', 'Safety')\n",
    "#         AND t4.city_id IN (6587,4230,5495,4272,4396)\n",
    "# ),\n",
    "# support AS (\n",
    "#     SELECT\n",
    "#         orders_raw.user_id,\n",
    "#         COUNT(DISTINCT support_id) cnt_support\n",
    "#     FROM\n",
    "#         orders_raw\n",
    "#     JOIN\n",
    "#         support_raw ON orders_raw.order_uuid = support_raw.order_uuid\n",
    "#     GROUP BY 1\n",
    "# )\n",
    "# SELECT\n",
    "#     ab.user_id,\n",
    "#     ab.has_treatment,\n",
    "#     appeals.cnt_appeals,\n",
    "#     reviews.cnt_reviews,\n",
    "#     support.cnt_support\n",
    "# FROM\n",
    "#     ab\n",
    "# LEFT JOIN\n",
    "#     appeals ON ab.user_id = appeals.user_id\n",
    "# LEFT JOIN\n",
    "#     reviews ON ab.user_id = reviews.user_id\n",
    "# LEFT JOIN\n",
    "#     support ON ab.user_id = support.user_id\n",
    "# \"\"\")\n",
    "\n",
    "# id = read_bq(\"\"\"\n",
    "\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e509f0",
   "metadata": {},
   "source": [
    "### Soft, medium, hard launch MDE calculating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5161f9b",
   "metadata": {},
   "source": [
    "- Жесткий (опасный) сценарий: берем все опасные и большие города и запускаем 50/50 на 2-4 недели\n",
    "- Средний сценарий: берем 50% опасных городов и запускаем 50/50 на 2-4 недели\n",
    "- Мягкий сценарий: берем 10% опасных городов и запускаем 30/70 на 2-4 недели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2317fa",
   "metadata": {},
   "source": [
    "#### Cities with metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebe377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = read_bq(\"\"\"\n",
    "WITH incidents AS (SELECT t1.city_id,\n",
    "                          t1.city_name,\n",
    "                          geo.country_id,\n",
    "                          geo.country_name,\n",
    "                          t1.monthly,\n",
    "                          SUM(rides)                                                                 AS rides,\n",
    "                          SUM(incidents)                                                             AS incidents,\n",
    "                          SUM(conf_incidents)                                                        AS confirmed_incidents,\n",
    "                          SUM(SAFE_DIVIDE(t1.incidents, t2.rides) * 100000) / COUNT(t1.city_id)      AS inc_rate,\n",
    "                          SUM(SAFE_DIVIDE(t1.conf_incidents, t2.rides) * 100000) / COUNT(t1.city_id) AS conf_inc_rate\n",
    "                   FROM (SELECT DATE_TRUNC(incident_date, MONTH)                              AS monthly,\n",
    "                                t1.city_id,\n",
    "                                t1.city_name,\n",
    "                                COUNT(redmine_id)                                             AS incidents,\n",
    "                                COUNT(IF(information_status = 'Confirmed', redmine_id, NULL)) AS conf_incidents\n",
    "                         FROM indriver-bi.safety.vw_safety_incidents_detail t1\n",
    "                                  JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                                       ON t1.city_id = t2.city_id\n",
    "                         WHERE incident_date >= '2025-01-01'\n",
    "                           AND t2.country_id IN (24, 12, 22, 25)\n",
    "                         GROUP BY 1, 2, 3) t1\n",
    "                            JOIN indriver-e6e40.heap.vw_macroregion_mapping geo\n",
    "                                 ON t1.city_id = geo.city_id\n",
    "                            LEFT JOIN (SELECT t1.city_id,\n",
    "                                              DATE_TRUNC(metric_date, MONTH) AS monthly,\n",
    "                                              SUM(rides_count)               AS rides\n",
    "                                       FROM indriver-bi.incity.tbl_incity_growth_metrics_detail t1\n",
    "                                                JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                                                     ON t1.city_id = t2.city_id\n",
    "                                       WHERE user_type = 'pass'\n",
    "                                         AND t2.country_id IN (24, 12, 22, 25)\n",
    "                                         AND metric_date >= '2025-01-01'\n",
    "                                       GROUP BY 1, 2) t2 ON t1.city_id = t2.city_id AND t1.monthly = t2.monthly\n",
    "                   GROUP BY 1, 2, 3, 4, 5),\n",
    "     metric AS (SELECT t1.city_id,\n",
    "                       DATE_TRUNC(metric_date, MONTH)                                       AS monthly,\n",
    "                       COUNT(DISTINCT t1.user_id)                                           AS total_users,\n",
    "                       COUNT(DISTINCT IF(t1.metric_date = t3.first_ride, t3.user_id, NULL)) AS newbies,\n",
    "                       COUNT(DISTINCT IF(t1.metric_date = t3.first_ride, t3.user_id, NULL)) /\n",
    "                       COUNT(DISTINCT t1.user_id)                                           AS share_of_newbies,\n",
    "                       SUM(rides_count)                                                     AS rides,\n",
    "                       SUM(gmv_unclean_usd)                                                 AS gmv,\n",
    "                       SUM(rides_count) / COUNT(t1.user_id)                                 AS avg_rides_by_one_user\n",
    "                FROM indriver-bi.incity.tbl_incity_growth_metrics_detail t1\n",
    "                         JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                              ON t1.city_id = t2.city_id\n",
    "                         LEFT JOIN (SELECT user_id,\n",
    "                                           MIN(metric_date) AS first_ride\n",
    "                                    FROM indriver-bi.incity.tbl_incity_growth_metrics_detail t1\n",
    "                                             JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "                                                  ON t1.city_id = t2.city_id\n",
    "                                    WHERE user_type = 'pass'\n",
    "                                      AND t2.country_id IN (24, 12, 22, 25)\n",
    "                                      AND metric_date >= '2023-01-01'\n",
    "                                    GROUP BY 1) t3 ON t1.user_id = t3.user_id\n",
    "                WHERE user_type = 'pass'\n",
    "                  AND t2.country_id IN (24, 12, 22, 25)\n",
    "                  AND metric_date >= '2025-01-01'\n",
    "                GROUP BY 1, 2)\n",
    "SELECT t1.city_id,\n",
    "       t1.city_name,\n",
    "       t1.country_id,\n",
    "       t1.country_name,\n",
    "       AVG(t1.rides)                           AS rides,\n",
    "       ROUND(AVG(t2.gmv), 2)                   AS gmv,\n",
    "       ROUND(AVG(t2.avg_rides_by_one_user), 2) AS avg_rides_by_one_user,\n",
    "       ROUND(AVG(t2.total_users), 0)           AS total_users,\n",
    "       ROUND(AVG(t2.share_of_newbies), 2)      AS share_of_newbies,\n",
    "       ROUND(AVG(t2.newbies), 0)               AS newbies,\n",
    "       ROUND(AVG(t1.incidents), 0)             AS incidents,\n",
    "       ROUND(AVG(t1.confirmed_incidents), 0)   AS confirmed_incidents,\n",
    "       ROUND(AVG(t1.inc_rate), 2)              AS inc_rate,\n",
    "       ROUND(AVG(t1.conf_inc_rate), 2)         AS conf_inc_rate,\n",
    "       '(' || (ROUND(AVG(t2.newbies) * 0.5 * 0.25, 0) || ', ' || ROUND(AVG(t2.newbies) * 0.5 * 0.3, 0) || ', ' ||\n",
    "               ROUND(AVG(t2.newbies) * 0.5 * 0.35, 0)) ||\n",
    "       ')'                                     AS loss_interval_newbies,\n",
    "       ROUND(AVG(t2.newbies) * 0.5 * 0.25, 0)  AS lower,\n",
    "       ROUND(AVG(t2.newbies) * 0.5 * 0.3, 0)   AS middle,\n",
    "       ROUND(AVG(t2.newbies) * 0.5 * 0.35, 0)  AS upper\n",
    "FROM incidents t1\n",
    "         JOIN metric t2\n",
    "              ON t1.city_id = t2.city_id AND t1.monthly = t2.monthly\n",
    "GROUP BY 1, 2, 3, 4\n",
    "\"\"\")\n",
    "\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70db248",
   "metadata": {},
   "source": [
    "#### MDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a766e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>city_id</th>\n",
       "      <th>cnt_appeals</th>\n",
       "      <th>cnt_reviews</th>\n",
       "      <th>cnt_support</th>\n",
       "      <th>incident_flag</th>\n",
       "      <th>composed_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302021628</td>\n",
       "      <td>4281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284485087</td>\n",
       "      <td>4374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>296076598</td>\n",
       "      <td>4519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55299089</td>\n",
       "      <td>4377</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173440584</td>\n",
       "      <td>4515</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  city_id  cnt_appeals  cnt_reviews  cnt_support  incident_flag  composed_metrics\n",
       "0  302021628     4281            0            0            0              0                 0\n",
       "1  284485087     4374            0            0            0              0                 0\n",
       "2  296076598     4519            0            0            0              0                 0\n",
       "3   55299089     4377            0            0            0              0                 0\n",
       "4  173440584     4515            0            0            0              0                 0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "list_of_cities = [\n",
    "    4199,4142,4197,4242,4143,4825,4404,4373,4198,4524,4200,4540, \n",
    "    4255,4257,4252,4231,4538,4258,4263,4229,4194,4193,4226,4516,4228,5543,4244,4266,4271,4545,4148,4559,4519,4154,4374,4155,4196,5512,4236,4549,4542,4515,5536,4272,4377,4163,4269,4233,4178,4267,5568,4241,4261,4144,4264,4180,4354,5528,4230,4281,4555,5504,4143,4825,4404,4373,4198,4524, \n",
    "    4524,4200,4540,4375,4376,4385,4225,5548,5512,4194,4236,4241,4515,4244,5504,4263,4148,4180,5543,4271,4272,4196,4542,4229,4381,4240,4267,4266,5568,4516,4275,4226,4228,4155\n",
    "]\n",
    "\n",
    "df = read_bq(f\"\"\"\n",
    "WITH users AS (SELECT user_id, MAX(city_id) AS city_id\n",
    "               FROM indriver-e6e40.ods_event_tracker.event\n",
    "               WHERE 1 = 1\n",
    "                 AND name IN (\n",
    "                   'client.verification_start.click'\n",
    "                   )\n",
    "                 AND event_dt_part >= '2025-04-01'\n",
    "                 AND city_id IN {tuple(set(list_of_cities))}\n",
    "               GROUP BY 1)\n",
    "   , incidents AS (SELECT DISTINCT pass_id AS agg_id\n",
    "                   FROM indriver-bi.safety.vw_safety_incidents_detail\n",
    "                   WHERE aggressor = 'Passenger'\n",
    "                     AND information_status IN ('Confirmed', 'Automated ML decision')\n",
    "                     AND pass_id IS NOT NULL\n",
    "                     AND city_id IN {tuple(set(list_of_cities))})      -- City\n",
    "   , appeals AS (SELECT target_id            user_id\n",
    "                      , COUNT(DISTINCT uuid) cnt_appeals\n",
    "                 FROM indriver-e6e40.ods_moderation_feed_red_pill.appeal t1\n",
    "                          LEFT JOIN\n",
    "                      indriver-e6e40.ods_ds_moderation_system_cdc.violation_review_v3 t3\n",
    "                      ON t1.uuid = JSON_EXTRACT_SCALAR(t3.payload\n",
    "                          , '$.uuid')\n",
    "                 WHERE DATE(t1.created_at) BETWEEN '2025-04-01' AND CURRENT_DATE()\n",
    "                   AND DATE(t3.export_raw_dt) BETWEEN '2025-04-01' AND CURRENT_DATE()\n",
    "--AND t1.initiator_id = 1 --жалоба от пассажира водиле\n",
    "                   AND t1.initiator_id = 0 --жалоба от водилы пассажиру\n",
    "                   AND t1.city_id IN\n",
    "                       {tuple(set(list_of_cities))}              -- City\n",
    "                   AND JSON_EXTRACT_SCALAR(t3.model\n",
    "                           , '$.result.top_category[0]') NOT IN\n",
    "                       ('CATEGORY_TEXT_NOT_RECOGNISED', 'CATEGORY_LOCATION_DISPUTE', 'CATEGORY_RIDE_REFUSAL',\n",
    "                        'CATEGORY_CANCELLED_BY_DRIVER_REQUEST', 'CATEGORY_BARGAINING_AFTER_ACCEPT',\n",
    "                        'CATEGORY_PASSENGER_WAS_LATE', 'CATEGORY_DRIVER_WAS_LATE', 'CATEGORY_APP_PROBLEM',\n",
    "                        'CATEGORY_POSITIVE_REVIEW', 'CATEGORY_DIFFERENT_CAR',\n",
    "                        'CATEGORY_DRIVER_REPORTED_CAR_MALFUNCTION', 'CATEGORY_STRANGER_IN_CAR')\n",
    "                 GROUP BY 1)\n",
    "   , reviews AS (SELECT target_id AS           user_id\n",
    "                      , COUNT(DISTINCT r.uuid) cnt_reviews\n",
    "                 FROM indriver-e6e40.ods_moderation_feed_red_pill.review r\n",
    "                          LEFT JOIN\n",
    "                      indriver-e6e40.ods_ds_moderation_system_cdc.violation_review_v3 src\n",
    "                      ON JSON_EXTRACT_SCALAR(src.payload\n",
    "                             , '$.uuid') = r.uuid\n",
    "                 WHERE DATE(r.created_at) BETWEEN '2025-04-01'\n",
    "                     AND CURRENT_DATE()\n",
    "                   AND DATE(src.export_raw_dt) BETWEEN '2025-04-01'\n",
    "                     AND CURRENT_DATE()\n",
    "                   AND rating\n",
    "                     < 5\n",
    "                   AND r.visibility_id = 1 --жалобы от водителя на пассажира\n",
    "                   AND r.city_id IN {tuple(set(list_of_cities))} -- City\n",
    "                   AND JSON_EXTRACT_SCALAR(src.model\n",
    "                           , '$.result.top_category[0]') NOT IN\n",
    "                       ('CATEGORY_TEXT_NOT_RECOGNISED', 'CATEGORY_DIRTY_CABIN', 'CATEGORY_SUSPICIOUS_AREA', --УДАЛИТЬ?\n",
    "                        'CATEGORY_ASSAULT', --УДАЛИТЬ?\n",
    "                        'CATEGORY_BARGAINING_AFTER_ACCEPT', 'CATEGORY_POSITIVE_REVIEW', 'CATEGORY_DIFFERENT_CAR',\n",
    "                        'CATEGORY_PASSENGER_REPORTED_CAR_MALFUNCTION', 'CATEGORY_NO_CHANGE',\n",
    "                        'CATEGORY_DANGEROUS_DRIVING', 'CATEGORY_CANCELLED_BY_PASSENGER_REQUEST')\n",
    "                 GROUP BY 1)\n",
    "   , orders_raw AS (SELECT DISTINCT order_uuid\n",
    "                                  , user_id\n",
    "                    FROM indriver-e6e40.emart.incity_detail\n",
    "                    WHERE created_date_order_part BETWEEN '2025-04-01'\n",
    "                        AND CURRENT_DATE()\n",
    "                      AND city_id IN {tuple(set(list_of_cities))})     -- City\n",
    "   , support_raw AS (SELECT DISTINCT t1.id              support_id\n",
    "                                   , LOWER(t4.order_id) order_uuid\n",
    "                     FROM indriver-e6e40.ods_customer_support.request t1\n",
    "                              JOIN\n",
    "                          indriver-bi.customer_service.tbl_customer_support_chats_just_detail t2\n",
    "                          ON t1.id = t2.request_id\n",
    "                              JOIN\n",
    "                          dwh-storage-327422.ods_customer_support.chat_request_entry t4 ON t1.id = t4.request_id\n",
    "                              JOIN\n",
    "                          (SELECT DISTINCT country_id\n",
    "                                         , country_name\n",
    "                                         , city_id\n",
    "                           FROM indriver-bi.heap.vw_geo_mapping) t3\n",
    "                          ON t2.country_name = t3.country_name\n",
    "                     WHERE DATE(t1.created_dt_part) BETWEEN '2025-04-01'\n",
    "                         AND CURRENT_DATE()\n",
    "                       AND t4.created_dt_part BETWEEN '2025-04-01'\n",
    "                         AND CURRENT_DATE()\n",
    "                       AND contact_category IS NOT NULL\n",
    "                       AND contact_reason IS NOT NULL\n",
    "                       AND who_contacts = 'Driver'\n",
    "                       AND contact_category IN ('Complaints against Passenger', 'Safety')\n",
    "                       AND t4.city_id IN {tuple(set(list_of_cities))}) -- City\n",
    "   , support AS (SELECT orders_raw.user_id\n",
    "                      , COUNT(DISTINCT support_id) cnt_support\n",
    "                 FROM orders_raw\n",
    "                          JOIN\n",
    "                      support_raw\n",
    "                      ON orders_raw.order_uuid = support_raw.order_uuid\n",
    "                 GROUP BY 1)\n",
    "SELECT t0.user_id,\n",
    "       t0.city_id,\n",
    "       COALESCE(t1.cnt_appeals, 0) AS cnt_appeals,\n",
    "       COALESCE(t2.cnt_reviews, 0) AS cnt_reviews,\n",
    "       COALESCE(t3.cnt_support, 0) AS cnt_support,\n",
    "       IF(agg_id IS NULL, 0, 1)    AS incident_flag\n",
    "FROM users t0\n",
    "         LEFT JOIN appeals t1 ON t0.user_id = t1.user_id\n",
    "         LEFT JOIN reviews t2 ON t0.user_id = t2.user_id\n",
    "         LEFT JOIN support t3 ON t0.user_id = t3.user_id\n",
    "         LEFT JOIN incidents t4 ON t0.user_id = t4.agg_id\n",
    "\"\"\")\n",
    "\n",
    "df['composed_metrics'] = df['cnt_appeals']+df['cnt_reviews']+df['cnt_support']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c710d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21349767469355427"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['incident_flag'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8876e66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.966634830603582"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['composed_metrics'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "expab.get_mde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed7afdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>event_dt_part</th>\n",
       "      <th>weekly</th>\n",
       "      <th>monthly</th>\n",
       "      <th>city_id</th>\n",
       "      <th>city_name</th>\n",
       "      <th>country_id</th>\n",
       "      <th>country_name</th>\n",
       "      <th>users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client.verification_start.click</td>\n",
       "      <td>2025-05-07</td>\n",
       "      <td>2025-05-04</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>4199</td>\n",
       "      <td>Lima</td>\n",
       "      <td>24</td>\n",
       "      <td>Peru</td>\n",
       "      <td>5288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client.verification_start.click</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4267</td>\n",
       "      <td>Arica</td>\n",
       "      <td>25</td>\n",
       "      <td>Chile</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client.verification_start.click</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4228</td>\n",
       "      <td>Tijuana</td>\n",
       "      <td>12</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>client.verification_start.click</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4229</td>\n",
       "      <td>Leon de los Aldama</td>\n",
       "      <td>12</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>client.verification_start.click</td>\n",
       "      <td>2025-05-03</td>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>4197</td>\n",
       "      <td>Bogota</td>\n",
       "      <td>22</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>3552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name event_dt_part      weekly     monthly  city_id           city_name  country_id country_name  users\n",
       "0  client.verification_start.click    2025-05-07  2025-05-04  2025-05-01     4199                Lima          24         Peru   5288\n",
       "1  client.verification_start.click    2025-04-04  2025-03-30  2025-04-01     4267               Arica          25        Chile    359\n",
       "2  client.verification_start.click    2025-04-04  2025-03-30  2025-04-01     4228             Tijuana          12       Mexico    875\n",
       "3  client.verification_start.click    2025-04-04  2025-03-30  2025-04-01     4229  Leon de los Aldama          12       Mexico    299\n",
       "4  client.verification_start.click    2025-05-03  2025-04-27  2025-05-01     4197              Bogota          22     Colombia   3552"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = read_bq(f\"\"\"\n",
    "SELECT name,\n",
    "       event_dt_part,\n",
    "       DATE_TRUNC(event_dt_part, WEEK)  AS weekly,\n",
    "       DATE_TRUNC(event_dt_part, MONTH) AS monthly,\n",
    "       t1.city_id,\n",
    "       t2.city_name,\n",
    "       t2.country_id,\n",
    "       t2.country_name,\n",
    "       COUNT(DISTINCT user_id)          AS users\n",
    "FROM indriver-e6e40.ods_event_tracker.event t1\n",
    "         JOIN indriver-e6e40.heap.vw_macroregion_mapping t2\n",
    "              ON\n",
    "                  t1.city_id = t2.city_id\n",
    "WHERE 1 = 1\n",
    "  AND name IN (\n",
    "    'client.verification_start.click'\n",
    "    )\n",
    "  AND event_dt_part >= '2025-02-01'\n",
    "  AND t1.city_id IN {tuple(set(list_of_cities))}\n",
    "GROUP BY 1, 2, 3, 4, 5, 6, 7, 8\n",
    "\"\"\")\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab06293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c93fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[df_sample['city_id'].isin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4669a319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>mde_abs</th>\n",
       "      <th>mde_%</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>Category</th>\n",
       "      <th>Metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4412</td>\n",
       "      <td>0.004</td>\n",
       "      <td>144.952</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.052</td>\n",
       "      <td>low</td>\n",
       "      <td>incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28290</td>\n",
       "      <td>0.002</td>\n",
       "      <td>57.243</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.052</td>\n",
       "      <td>low</td>\n",
       "      <td>incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56580</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40.477</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.052</td>\n",
       "      <td>low</td>\n",
       "      <td>incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120234</td>\n",
       "      <td>0.001</td>\n",
       "      <td>27.767</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.052</td>\n",
       "      <td>low</td>\n",
       "      <td>incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4412</td>\n",
       "      <td>0.022</td>\n",
       "      <td>44.390</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.289</td>\n",
       "      <td>low</td>\n",
       "      <td>composed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28290</td>\n",
       "      <td>0.009</td>\n",
       "      <td>17.530</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.289</td>\n",
       "      <td>low</td>\n",
       "      <td>composed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56580</td>\n",
       "      <td>0.006</td>\n",
       "      <td>12.396</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.289</td>\n",
       "      <td>low</td>\n",
       "      <td>composed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120234</td>\n",
       "      <td>0.004</td>\n",
       "      <td>8.503</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.289</td>\n",
       "      <td>low</td>\n",
       "      <td>composed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4412</td>\n",
       "      <td>0.010</td>\n",
       "      <td>69.256</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.128</td>\n",
       "      <td>low</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28290</td>\n",
       "      <td>0.004</td>\n",
       "      <td>27.350</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.128</td>\n",
       "      <td>low</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56580</td>\n",
       "      <td>0.003</td>\n",
       "      <td>19.339</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.128</td>\n",
       "      <td>low</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120234</td>\n",
       "      <td>0.002</td>\n",
       "      <td>13.267</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.128</td>\n",
       "      <td>low</td>\n",
       "      <td>reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size  mde_abs   mde_%  alpha  beta  mean   std Category    Metric\n",
       "0         4412    0.004 144.952  0.100 0.200 0.003 0.052      low  incident\n",
       "1        28290    0.002  57.243  0.100 0.200 0.003 0.052      low  incident\n",
       "2        56580    0.001  40.477  0.100 0.200 0.003 0.052      low  incident\n",
       "3       120234    0.001  27.767  0.100 0.200 0.003 0.052      low  incident\n",
       "0         4412    0.022  44.390  0.100 0.200 0.049 0.289      low  composed\n",
       "1        28290    0.009  17.530  0.100 0.200 0.049 0.289      low  composed\n",
       "2        56580    0.006  12.396  0.100 0.200 0.049 0.289      low  composed\n",
       "3       120234    0.004   8.503  0.100 0.200 0.049 0.289      low  composed\n",
       "0         4412    0.010  69.256  0.100 0.200 0.014 0.128      low   reviews\n",
       "1        28290    0.004  27.350  0.100 0.200 0.014 0.128      low   reviews\n",
       "2        56580    0.003  19.339  0.100 0.200 0.014 0.128      low   reviews\n",
       "3       120234    0.002  13.267  0.100 0.200 0.014 0.128      low   reviews"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "segment = ['hard', 'medium', 'low']\n",
    "hard = [4199,4142,4197,4242,4143,4825,4404,4373,4198,4524,4200,4540]\n",
    "medium = [4255,4257,4252,4231,4538,4258,4263,4229,4194,4193,4226,4516,4228,5543,4244,4266,4271,4545,4148,4559,4519,4154,4374,4155,4196,5512,4236,4549,4542,4515,5536,4272,4377,4163,4269,4233,4178,4267,5568,4241,4261,4144,4264,4180,4354,5528,4230,4281,4555,5504,4143,4825,4404,4373,4198,4524]\n",
    "low = [4524,4200,4540,4375,4376,4385,4225,5548,5512,4194,4236,4241,4515,4244,5504,4263,4148,4180,5543,4271,4272,4196,4542,4229,4381,4240,4267,4266,5568,4516,4275,4226,4228,4155]\n",
    "\n",
    "df_sample['event_dt_part'] = pd.to_datetime(df_sample['event_dt_part'])\n",
    "\n",
    "sizes\n",
    "res = pd.DataFrame()\n",
    "\n",
    "for category in segment:\n",
    "    if category == 'hard':\n",
    "        df_group = df[df['city_id'].isin(hard)]\n",
    "        daily = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(hard))].groupby(['event_dt_part'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        weekly = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(hard))].groupby(['weekly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        month = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(hard))].groupby(['monthly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        sizes = [int(daily), int(weekly), int(weekly)*2, int(month)]\n",
    "    elif category == 'medium':\n",
    "        df_group = df[df['city_id'].isin(medium)]\n",
    "        daily = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(medium))].groupby(['event_dt_part'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        weekly = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(medium))].groupby(['weekly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        month = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(medium))].groupby(['monthly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        sizes = [int(daily), int(weekly), int(weekly)*2, int(month)]\n",
    "    elif category == 'low':\n",
    "        df_group = df[df['city_id'].isin(low)]\n",
    "        daily = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(low))].groupby(['event_dt_part'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        weekly = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(low))].groupby(['weekly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        month = df_sample[(df_sample['event_dt_part']<= '2025-05-20')&(df_sample['city_id'].isin(low))].groupby(['monthly'], as_index=False)['users'].sum()['users'].mean().round(0)*0.4\n",
    "        sizes = [int(daily), int(weekly), int(weekly)*2, int(month)]\n",
    "\n",
    "\n",
    "    mean = df_group['composed_metrics'].mean()\n",
    "    std = df_group['composed_metrics'].std()\n",
    "    sample_size = len(df_group)\n",
    "    mde_comp = expab.get_mde(mean, std, sizes, alpha=0.1, beta=0.2)\n",
    "\n",
    "    mde_comp['mean'] = mean\n",
    "    mde_comp['std'] = std\n",
    "    mde_comp['Category'] = category\n",
    "    mde_comp['Metric'] = 'composed'\n",
    "\n",
    "    mean = df_group['cnt_reviews'].mean()\n",
    "    std = df_group['cnt_reviews'].std()\n",
    "    sample_size = len(df_group)\n",
    "    mde = expab.get_mde(mean, std, sizes, alpha=0.1, beta=0.2)\n",
    "\n",
    "    mde['mean'] = mean\n",
    "    mde['std'] = std\n",
    "    mde['Category'] = category\n",
    "    mde['Metric'] = 'reviews'\n",
    "\n",
    "    mean = df_group['incident_flag'].mean()\n",
    "    std = df_group['incident_flag'].std()\n",
    "    sample_size = len(df_group)\n",
    "    mde_inc = expab.get_mde(mean, std, sizes, alpha=0.1, beta=0.2)\n",
    "\n",
    "    mde_inc['mean'] = mean\n",
    "    mde_inc['std'] = std\n",
    "    mde_inc['Category'] = category\n",
    "    mde_inc['Metric'] = 'incident'\n",
    "\n",
    "    res = pd.concat([res, mde_inc, mde_comp, mde])\n",
    "\n",
    "# res.pivot_table(columns='Category', index=['sample_size', 'Metric'], values=['mde_%', 'mde_abs', ''])\n",
    "res[res['Category']=='low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8a0cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-72.86003869897723\n",
      "-71.13163972286374\n",
      "-69.37731839953902\n"
     ]
    }
   ],
   "source": [
    "print((5.891-21.706)/21.706*100)\n",
    "print((6.875-23.815)/23.815*100)\n",
    "print((8.503-27.767)/27.767*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50c1eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.900995010000009"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-(1-(0.05/5))**5)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e9a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa97e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53218d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
